%                                                                 aa.dem
% AA vers. 9.1, LaTeX class for Astronomy & Astrophysics
% demonstration file
%                                                       (c) EDP Sciences
%-----------------------------------------------------------------------
%
%\documentclass[referee]{aa} % for a referee version
%\documentclass[onecolumn]{aa} % for a paper on 1 column  
%\documentclass[longauth]{aa} % for the long lists of affiliations 
%\documentclass[letter]{aa} % for the letters 
%\documentclass[bibyear]{aa} % if the references are not structured 
%                              according to the author-year natbib style

%
\documentclass{aa}  

%
\usepackage{graphicx,multirow,xcolor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{txfonts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[options]{hyperref}
\usepackage[hidelinks]{hyperref}
% To add links in your PDF file, use the package "hyperref"
% with options according to your LaTeX or PDFLaTeX drivers.
%
\usepackage{showyourwork}
\usepackage{comment}
\usepackage[switch,modulo]{lineno}
\linenumbers
%\modulolinenumbers[10]

\begin{document} 


   %\title{Developing a pipeline to predict high-redshift Radio Galaxies}
   \title{Selection of Powerful Radio Galaxies with Machine Learning}

   %\subtitle{Selection of Powerful Radio AGN with Machine Learning}

   \author{R. Carvajal\inst{1, 2}
          \and
          I. Matute\inst{1, 2}
          \and
          J. Afonso\inst{1, 2}
          \and
          R. P. Norris\inst{3, 4}
          \and
          K. J. Luken\inst{3, 5}
          \and
          P. S\'{a}nchez-S\'{a}ez\inst{6}
          \and
          P. Cunha\inst{7, 8}
          \and
          A. Humphrey\inst{7, 9}
          \and
          H. Messias\inst{10, 11}
          \and
          S. Amarantidis\inst{12, 1}
          \and
          D. Barbosa\inst{1, 2}
          \and
          H. A. Cruz\inst{13}
          \and
          H. Miranda\inst{1, 2}
          \and
          A. Paulino-Afonso\inst{7}
          \and
          C. Pappalardo\inst{1, 2}
          }

   \institute{Instituto de Astrof\'isica e Ci\^encias do Espa\c{c}o, Universidade de Lisboa, OAL, Tapada da Ajuda, 1349-018 Lisbon, Portugal \email{racarvajal@fc.ul.pt}
         \and
             Departamento de F\'isica, Faculdade de Ci\^encias, Universidade de Lisboa, EdifÃ­cio C8, Campo Grande, 1749-016 Lisbon, Portugal
        \and
            School of Science, Western Sydney University, Locked Bag 1797, Penrith, NSW 2751, Australia
        \and
            CSIRO Space \& Astronomy, Australia Telescope National Facility, P.O. Box 76, Epping, NSW 1710, Australia
        \and
            CSIRO Data61, P.O. Box 76, Epping, NSW 1710, Australia
        \and
            European Southern Observatory, Karl-Schwarzschild-Stra{\ss}e 2, 85748 Garching bei M\"{u}nchen, Germany.
        \and
            Instituto de Astrof\'isica e Ci\^encias do Espa\c{c}o, Universidade do Porto, CAUP, Rua das Estrelas, 4150-762 Porto, Portugal
        \and
            Departamento de F\'isica e Astronomia, Faculdade de Ci\^encias, Universidade do Porto, Rua do Campo Alegre 687, 4169-007 Porto, Portugal
        \and
            DTx - Digital Transformation CoLAB, Building 1, Azur\'em Campus, University of Minho, 4800-058 Guimar\~{a}es, Portugal
        \and
            Joint ALMA Observatory, Alonso de C\'ordova 3107, Vitacura 763-0355, Santiago, Chile
        \and
            European Southern Observatory, Alonso de C\'ordova 3107, Vitacura, Casilla 19001, Santiago de Chile, Chile
        \and
            Institut de Radioastronomie Millim\'etrique (IRAM), Avenida Divina Pastora 7, Local 20, 18012 Granada, Spain
        \and
            Closer Consultoria Lda, Torre 1, Av. Eng. Duarte Pacheco 15, 1070-101 Lisboa, Portugal
             }

   \date{Received ; accepted }

% \abstract{}{}{}{}{} 
% 5 {} token are mandatory
 
  \abstract
  % context heading (optional)
  % {} leave it empty if necessary  
   {The study of Active Galactic Nuclei (AGN) is fundamental to discern the formation and growth of supermassive black holes (SMBHs) and their connection with star-formation history and galaxy evolution. Powerful radio emission from AGN traces the largest structures in the Universe and, given their associated radiative and kinetic energy, is a prime feedback candidate to understand correlations between properties of the SMBH and the host galaxy. Much is left to understand about mechanisms and conditions that trigger jetted radio emission in AGN. To understand their role in cosmic evolution, we need to extract demographic estimates along cosmic times. These studies are relevant in the Epoch of Reionization (EoR; ${z{>}6-7}$), when the first signs of the SMBH-host connection are thought to start. But few AGN have been identified in the EoR, of which only a small fraction have radio detections.
   }
  % aims heading (mandatory)
   {The goal of this paper is to develop a method to predict the AGN nature of a source, its radio detectability and its redshift. The development of such method will increase the number of radio AGN across cosmic times, allowing us to improve our knowledge of accretion power into SMBH, the origin and triggers of radio emission and its impact on galaxy evolution.   
   }
  % methods heading (mandatory)
   {We developed a pipeline of three stacked Machine Learning (ML) models which predicts the probability of a source to be an AGN and to be detected in specific radio surveys. Also, it can estimate redshift values for the predicted radio-detectable AGN. These ML models, which combine predictions from the algorithms \texttt{Random Forest}, \texttt{Extra Trees}, \texttt{Gradient Boosting Classifier}, \texttt{Gradient Boosting Regressor}, \texttt{CatBoost}, and \texttt{XGBoost}, have been trained with multi-wavelength data from near infrared-selected sources in The Hobby-Eberly Telescope Dark Energy Experiment (HETDEX) Spring Field. Training, testing, calibration and validation of the prediction pipeline were carried out in the HETDEX Spring field. Further validation was performed on near infrared-selected sources in the Stripe 82 field.
   }
  % results heading (mandatory)
   {In the HETDEX Spring field validation sub-set, our pipeline recovers $96\%$ of the initially-labelled AGN and, from the AGN candidates, we recover $50\%$ of previously-detected radio sources. For Stripe 82, these numbers are $94\%$ and $55\%$, respectively. Feature importance analysis stresses the significance of near- and mid-IR colours not only to select AGN but also to identify their radio nature and redshift value.
   }
  % conclusions heading (optional), leave it empty if necessary 
   {The use of generalised stacking in ML models shows a perceivable improvement in the prediction power of our pipeline. This technique is pertinent for the radio detection prediction where it is the first time, to our best knowledge, that has been achieved. Tree-based ML models (in contrast to Deep Learning techniques) facilitate the analysis of the impact that features have on the predictions. This prediction can give insight into the potential physical interplay between the properties of radio AGN (e.g. mass of black hole, accretion rate, etc).}

   \keywords{Galaxies: active -- Radio continuum: galaxies -- Galaxies: high-redshift -- Catalogs -- Methods: statistical.
               }

   \maketitle
%
%-------------------------------------------------------------------

\section{Introduction}\label{sec:introduction}

Active Galactic Nuclei (AGN) are instrumental to determine the nature, growth, and evolution of supermassive black holes (SMBHs). Their strong emission allows us to study the close environment within the hosting galaxies and, at a larger scale, the intergalactic medium \citep[e.g.][]{2017A&ARv..25....2P, 2022MNRAS.516.5775B}.

Although radio emission can trace high star-formation in galaxies, above certain luminosities \citep[e.g. $\mathrm{log}L_{1.4\mathrm{GHz}} {>} 25\,\mathrm{W\, Hz^{-1}}$,][]{2021MNRAS.503.1780J} it is a prime tracer of the powerful jet emission originated  close to the SMBH in AGN \citep[Radio Galaxies,][]{2014ARA&A..52..589H}. Traditionally, these powerful radio galaxies (RGs) were used to pinpoint AGN activity but have been superseded in the last decades by optical, NIR, and X-ray surveys. In fact, RGs in the high redshift Universe ($z > 2$) have been identified and studied only through the follow-up of AGN selected at shorter wavelengths \citep[optical, NIR, millimetre, and X-rays, e.g.][]{2006ApJ...652..157M, 2020A&A...637A..84P, 2021MNRAS.501.3833D}.
The landscape is quickly changing and the advent of new radio instruments and surveys has allowed the detection of larger numbers of RGs \citep[e.g.][]{2018MNRAS.475.3429W, 2020A&A...642A.107C}. Some of these surveys are: the Faint Images of the Radio Sky at Twenty-Centimetres \citep[FIRST;][]{2015ApJ...801...26H}, the Evolutionary Map of the Universe \citep[EMU;][]{2011PASA...28..215N}, the Very Large Array Sky Survey \citep[VLASS;][]{2020RNAAS...4..175G}, and the LOFAR Two-metre Sky Survey \citep[LoTSS;][]{2019A&A...622A...1S}. 

One of the ultimate goals is to detect powerful RGs in the Epoch of Reionization (EoR) that could be used to trace the neutral gas distribution during this critical phase of the Universe \citep[e.g.][]{2004NewAR..48.1029C, 2013MNRAS.435..460J}. Simulations  have shown that as much as a few hundreds of RGs per deg$^2$ could be present in the EoR \citep[][]{2019MNRAS.485.2694A, 2019MNRAS.482....2B, 2021MNRAS.503.3492T} and detectable with present and future deep observations --e.g. Square Kilometre Array (SKA), which is projected to have $\mu$Jy sensitivity levels \citep{2015aska.confE..67P}--. These expectations collide with the most recent observational compilations \citep[e.g.][]{2020ARA&A..58...27I, 2020MNRAS.494..789R, sarah_e_i_bosman_2022_6039724, 2022arXiv221206907F}, where only around $300$ AGN have been confirmed to exist at redshifts higher than $z{\sim}6$ over thousands of sq. degrees. This disagreement might well be just due to the limits of current selection techniques and a better understanding of the high-$z$ RGs might require alternative approaches.

The selection of AGN candidates has had success in the X-rays and radio wavebands as they dominate the emission above certain luminosities. Unfortunately, deep X-ray surveys are limited in area and only of the order of $10\%$ of AGN have strong radio emissions linked to jets at any given time with variations, up to $30\%$, correlated to optical and X-ray luminosities \citep[e.g.][]{1993MNRAS.263..461P, 1994ApJ...430..533D, 2019NatAs...3...48S, 2021MNRAS.506.5888M}. In fact, the precise triggering mechanisms and duty cycle for jetted emission are still unclear \citep{2015aska.confE..71A, 2022MNRAS.510.1163P}.

Radio colours and spectral indices, have been proposed as a tool to identify the origin of the radio emission \citep{2000A&A...354..423L}, while recently \citet{2019A&A...622A..17S} have shown that a  slight correlation exists between radio spectral index and radio luminosity for AGN.
The largest number of candidates have been selected through the compilation of multi-wavelength energy distributions (SED) for millions of sources \citep{2018ARA&A..56..625H, 2020PhDT.........3P}.  Of particular relevance for AGN are the near-IR colours where \textit{Spitzer} \citep{2004ApJS..154....1W} and especially  the Wide-field Infrared Survey Explorer \citep[\textit{WISE};][]{2010AJ....140.1868W} have opened a window for the detection of AGN over the whole sky, including the elusive fraction of heavily obscured ones \citep[e.g.][]{2012ApJ...753...30S, 2012MNRAS.426.3271M, 2017ApJ...836..182J, 2018ApJS..234...23A, 2021ApJ...922..179B}. The success of the NIR selection depends, to a grand extent, on a less severe effect of dust extinction at those wavelengths and on the characteristic power-law emission of the dust heated by the AGN. Radio wavelengths are not affected by dust obscuration and are, in principle, ideal to select AGN.

Regardless of the radio nature of an AGN, extensive spectroscopic follow-up measurements have allowed the confirmation of the estimated redshifts for more than $800\,000$ AGN over large areas of the sky \citep{2021arXiv210512985F}. Spectroscopic surveys have also contributed on their own to the detection of AGN activity through the analysis of line ratio as is the case of the BPT diagram \citep*{1981PASP...93....5B}. However, their determination can take long integration times and high-quality observations, which are not always available for all sources, rendering them not suited for large-sky catalogues. Photometric classification and redshifts (photo-$z$), where the multi-wavelength photometry of the source is compared to known objects or templates, are a viable option to understand the source nature and distribution across cosmic time \citep{1957AJ.....62....6B, 2019NatAs...3..212S}. Using this method, redshift estimations have been obtained for galaxies \citep[e.g.][]{2021A&A...654A.101H}, and AGN \citep[e.g.][]{2017ApJ...850...66A}. Traditional photo-$z$ estimations are computationally expensive since they rely on the best solution search within a grid of templates and models. In general, the computational cost scales linearly with the template grid and the number of sources and therefore becomes unfeasible for large catalogues (${\gtrsim}10^{7}$). Thus, to reach a result in such cases, High-Performance Computing facilities are needed as shown, for instance, from the tests done by \citet{2021ApJ...916...43G}. One way to avoid the use of high computational costs is through the use of drop-out techniques where a specific colour is used to select sources in a given redshift interval and therefore highly efficient at generating rough redshifts of large samples. The drop-out technique has been mainly used to generate and study high-redshift sources or candidates that, otherwise, would not have enough information to produce a precise redshift value \citep[e.g.][]{2020ApJ...902..112B, 2020A&A...633A.160C, 2022arXiv221106915S}.

Alternative statistical and computational methods can analyse thousands or millions of elements and find relevant trends among their properties. One branch of these techniques can, using previously-modelled data, predict the behaviour new data will have --i.e. the values of their properties--. This is what has been called Machine Learning \citep[ML;][]{5392560}.

In Astronomy, ML has been used with much success in a wide range of subjects, such as redshift determination, morphological classification, emission prediction, anomaly detection, observations planning, and more \citep[e.g.][]{2010IJMPD..19.1049B, 2019arXiv190407248B}. Traditional ML models are, in general, only fed with measurements and not with no physical assumptions \citep{Desai2021}, and they do not need to check the consistency of the predictions or the results they provide. As a consequence, prediction times of traditional ML methods are typically less than physically-based methods.

Despite the large number of applications it might have, one important criticism that ML has received is related to the lack of interpretability --or explainability, as it is called in ML jargon-- of the derived models, trends, and correlations. Most ML models, after taking a series of measurements and properties as input, deliver a prediction of a different property. But they cannot provide coefficients or an analytical expression, that might allow to find an equation for future predictions \citep{goebel2018explainable}. One important counter-example of this fact is the use of Symbolic Regression \citep[e.g.][]{2020arXiv200611287C, 2021ApJ...915...71V}. This implies that, for most ML models, it is not a simple task to understand which properties, and to what extent, help predict and interpret another attribute. This fact hinders our capability to understand the results in physical terms.

Recent work has been done to overcome the lack of explainability in ML models. The most widely used assessment is done with Feature Importance \citep{10.1007/978-3-030-10925-7_40, 9007737}, both global and local \citep{Saarela2021}. Game theory based analyses, like the Shapley analysis \citep{Shapley_article}, have also been used to understand feature importance in Astrophysics \citep[e.g.][]{2021MNRAS.507.1468M, 2022MNRAS.515.5285D, 2021Galax...9...86C, 2022MNRAS.509.3441A, 2022MNRAS.516.4716A}. 

A further complication is that astronomical data can be very heterogeneous. 
Surveys and instruments gather data from many different areas in the sky with very different sensitivities and observational properties. This heterogeneity severely complicates most astronomical analyses, but in particular ML methods, as they are driven most of the time completely by the data. This is expected to be alleviated in next-generation observatories and surveys --e.g. SKA, LSST, etc.-- where observations will be carried out homogeneously over very large areas.

Future observatories and surveys will deliver immense datasets. One option to analyse such observations (in particular, in radio wavelengths) is through the use of object-finding procedures \citep[e.g.][]{1996A&AS..117..393B, 2020ASPC..527..461B, 2012MNRAS.425..979H, 2015ascl.soft02007M, 2018A&C....23...92C} and then, using additional tools, confirm the nature of such objects. The use of object-detection tools over large areas of the sky will have a very high cost, both in time and in computational resources \citep{2019MNRAS.484.2793V}. An alternative to this method is using already-available multi-wavelength data to determine, as precisely as needed, the likelihood of a known source to be detected in a specific wavelength as a consequence of hosting an AGN \citep[see, for instance,][]{2022arXiv221201915P}. As mentioned earlier, only a small fraction of AGN can be detected in radio wavelengths \citep[radio-loud AGN][]{1993MNRAS.263..461P, 1994ApJ...430..533D, 2021MNRAS.506.5888M}. Thus, a random selection of AGN would lead to close to $10\%$ rate of radio-detectable AGN\footnote{We will call this random selection a `no-skill' selection method.}. With the use of existing data, ML can help speeding this process up via the training of models that can detect sources in large radio surveys \citep[see, for an example of the efforts done to achieve this goal,][]{2015PASA...32...37H, 2021MNRAS.500.3821B}.

In this work, we aim to identify candidates of high-redshift radio-detectable AGN which can be extracted from heterogeneous large-area surveys using a fraction of the time of traditional techniques. We developed a series of ML models to predict, separately, the detection of AGN, the detection of the radio signal from AGN, and the redshift values of radio-detectable AGN. Furthermore, we tested the performance of these models without applying a large number of previous cleaning steps (e.g. removing sources or reducing the studied area to increase the fraction of sources fully detected on every survey or homogenise measured ranges of observed properties for different classes of sources), which reduce considerably the size of the training sets. The compiled catalogue of candidates can help using data from future large-sky surveys more efficiently, as observational and analytical efforts can be focused on the areas in which AGN have been predicted to exist. This work is an extension of the results presented by \citet{2021Galax...9...86C}, who produced and analysed the predictions of photometric redshifts for confirmed AGN in a specific area of the sky. We seek to test such models by training them in an area with homogeneous coverage in several surveys and applying them in a different area with data that is not necessarily of the same quality in a sequential way, taking the results from the previous model as input. The approach taken by this work might help filling the gap of fast and reliable radio detection determination. The analysis of radio detections has been centred around the assumption of existing radio emission. In our case, we want to take this determination one step back and estimate whether there might, or not, be a radio measurement.

The structure of this article is as follows. In Sect.~\ref{sec:data}, we present the data and its preparation for ML training. The selection of models and the metrics used to assess their results are shown in Sect.~\ref{sec:ML_training}. In Sect.~\ref{sec:results}, the results of model training and validation are shown as well as the predictions using the ML pipeline for radio AGN detections. We present the discussion of our results in Sect.~\ref{sec:discussion}. Finally, in Sect.~\ref{sec:summary_conclusions}, we summarise our work.

%--------------------------------------------------------------------
\section{Data}\label{sec:data}

A large area with deep and homogeneous quality radio observations is needed to train and validate our models and predictions for RGs with already existent observations. As training field we selected the area of the Hobby-Eberly Telescope Dark Energy Experiment Spring Field \citep[HETDEX;][]{2008ASPC..399..115H} covered by the first data release of the LOFAR Two-metre Sky Survey \citep[LoTSS-DR1;][]{2019A&A...622A...1S}. The LoTSS-DR1 survey covers $424\, \mathrm{deg}^{2}$ in the HETDEX Spring field (hereafter, HETDEX field) with LOFAR \citep{2013A&A...556A...2V} $150\, \mathrm{MHz}$ observations that have a median sensitivity of $71\, \mu\mathrm{Jy}/\mathrm{beam}$. HETDEX provides as well multi-wavelength homogeneous coverage as described below.

In order to test the performance of the models when applied to different areas of the sky, and with different coverages from radio surveys, we have selected the Sloan Digital Sky Survey \citep[SDSS,][]{2000AJ....120.1579Y} Stripe 82 Field \citep[S82,][]{2014ApJ...794..120A, 2014ApJS..213...12J}. For S82, we collected data from the same surveys as with the HETDEX (see the following section) field but with one important caveat: no LoTSS-DR1 data is available in the field and, thus, we gathered the radio information from the VLA SDSS Stripe 82 Survey \citep[VLAS82;][]{2011AJ....142....3H}. VLAS82 covers an area of $92\, \mathrm{deg}^{2}$ with a median rms noise of $52\,\mu\mathrm{Jy}/\mathrm{beam}$ at 1.4$\,$GHz. We have selected the Stripe 82 field (and, in particular, the area covered by VLAS82) given that it presents deep radio observations but taken with a different instrument than LOFAR. This difference allows us to test the suitability of our models and procedures in conditions that are different from the training circumstances.


\subsection{Data collection}\label{sec:data_collection}

The base survey from which all the studied sources have been drawn is the CatWISE2020 catalogue \citep[CW;][]{2021ApJS..253....8M}. It lists NIR-detected elements selected from WISE \citep{2010AJ....140.1868W} and NEOWISE \citep{2011ApJ...731...53M, 2014ApJ...792...30M} over the entire sky at $3.4$ and $4.6$ $\mu$m (W1 and W2 bands, respectively). This catalogue includes sources detected at 5$\sigma$ in either of the used bands (i.e. W1${\sim} 17.43$ and W2${\sim} 16.47$ $\mathrm{mag}_{\mathrm{Vega}}$ respectively). The HETDEX field contains $15\,136\,878$ sources listed in CW. Conversely, in the Stripe 82 field, there are $3\,590\,306$ of them.

Multi-wavelength counterparts for CW sources were found on other catalogues applying a 5\arcsec search criteria. These catalogues include Pan-STARRS DR1 \citep[PS1;][]{2016arXiv161205560C, 2020ApJS..251....7F}, 2MASS All-Sky \citep[2M;][]{2006AJ....131.1163S, 2003tmc..book.....C, 2003yCat.2246....0C}, and AllWISE \citep[AW;][]{2013wise.rept....1C}\footnote{For the purposes of the analyses, and except when clearly stated otherwise, all photometric measurements were converted to AB magnitudes.}. The adopted search radius corresponds to the distance that has been used by \citet{2010AJ....140.1868W} to match radio sources to Pan-STARRS and WISE observations. Nevertheless, the source density of the radio (LOFAR and VLA) and 2MASS catalogues imply a low statistical ($<1\%$) spurious counterpart association, this is not the case for PS1, where the source density is higher. For this reason, and to mantain a statistically low spurious association between CW and PS1, we limited our search radius to 1.1\arcsec. This distance corresponds to the smallest Point-Spread Function (PSF) size of the bands included in PS1 \citep{2016arXiv161205560C}.

For the purposes of this work, observations in LoTSS and VLAS82 are only used to determine whether a source is radio detected, or not. In particular, no check has been performed on whether a selected source is extended or not in any of the radio surveys. A single boolean feature is created from the radio measurements (see Sect.~\ref{sec:feature_creation}) and no further analyses were performed regarding the detection levels that might be found.

Additionally, we have discarded the measurement errors of all bands. Traditionally, ML algorithms cannot incorporate uncertainties in a straightforward way and, thus, we opted to avoid attempting to use them for training \citep[for some examples on how they can be incorporated in astrophysically motivated ML studies, see][]{2008ApJ...683...12B, 2019AJ....157...16R, 2022AJ....164....6S}. Furthermore, \citet{2022arXiv220913074H} have shown that, in specific cases, the inclusion of measurement errors does not add new information to the training of the models and can be even detrimental to the prediction metrics. The degradation of the model by including uncertainties can likely be related to the fact that, by virtue of the large number of sources included in the training stages, the uncertainties are already encoded in the dataset in the form of scatter.

Following the same argument of measurement errors, upper limit values have been removed and a missing value is assumed instead. In general, ML methods (and their underlying statistical methods) cannot work with catalogues that have empty entries \citep{allison2001missing}. For that reason, we have used single imputation \citep[a review on the use of this method in astronomy can be seen in][]{ChattopadhyayData} to replace these missing values, and those fainter than $5{-}\sigma$ limits, with meaningful quantities that represent the lack of a measurement. 
We have opted for the inclusion of the same $5{-}\sigma$ limiting magnitudes as the value to impute with. 
This method of imputation with some variations, has been successfully applied and tested, recently, by \citet{2020MNRAS.498.1750A, 2021Galax...9...86C, 2022MNRAS.512.2099C}, and \citet{2022MNRAS.514....1C}.

In this way, observations from $12$ non-radio bands were gathered (as listed in Table~\ref{table:used_bands}). 
The magnitude density distribution for the sample from the HETDEX and S82 fields, without any imputation, is shown in Fig.~\ref{fig:hists_bands_nonimp_HETDEX_S82}. After imputation, the distribution of magnitudes changes, as shown in Fig.~\ref{fig:hists_bands_HETDEX_S82}. Each panel of the figure shows the number of sources which have a measurement above its $5{-}\sigma$ limit in such band. 
Additionally, a representation of the observational $5{-}\sigma$ limits of the bands and surveys used in this work is presented in Fig.~\ref{fig:surveys_depth_HETDEX}. 
It is worth noting the depth difference between VLAS82 and LoTSS-DR1 is ${\sim}1.5$\,mag for a typical synchrotron emitting source ($F_\nu \propto \nu^{\alpha}$ with $\alpha {=}-0.8)$, allowing the latter survey reach fainter sources.% We elaborate on the implications of this difference in Sect.~??.

\begin{figure}
  \centering
  \script{fig_nonimputed_mags_histograms.py}
  \includegraphics[width=0.99\columnwidth]{figures/hists_bands_norm_HETDEX_S82_non_imputed.pdf}
  \caption{Histograms of base collected, non-imputed, non-radio bands for HETDEX (clean, background histograms) and Stripe 82 (empty, brown histograms). Each panel shows the distribution of measured magnitudes of detected sources divided by the total area of the field. Dashed, vertical lines represent the $5{-}\sigma$ magnitude limit for each band. The number in the upper right corner of each panel shows the number of measured magnitudes included in their corresponding histogram.}
  \label{fig:hists_bands_nonimp_HETDEX_S82}
\end{figure}

\begin{figure}
   \centering
   \script{fig_imputed_mags_histograms.py}
   \includegraphics[width=0.99\columnwidth]{figures/hists_bands_norm_HETDEX_S82_imputed.pdf}
   \caption{Histograms of base collected non-radio bands for HETDEX (clean, background histograms) and Stripe 82 (empty, brown histograms) fields. Description as in Fig.~\ref{fig:hists_bands_nonimp_HETDEX_S82}. The number in the upper right corner of each panel shows the number of sources with magnitudes originally measured above the $5{-}\sigma$ limit included in their corresponding histogram for each field.}
   \label{fig:hists_bands_HETDEX_S82}
\end{figure}

\begin{table}
\setlength{\tabcolsep}{2pt}
\caption{Bands available for model training in our dataset}             % title of Table
\label{table:used_bands}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.80\columnwidth}{!}{
\begin{tabular}{c c}        % centered columns (2 columns)
\hline\hline                 % inserts double horizontal lines   
Survey             & Band (Column name)\tablefootmark{a} \\
\hline
\multirow{2}{*}{Pan-STARRS (PS1)}       & g (\texttt{gmag}), r (\texttt{rmag}), i (\texttt{imag}),  \\
                 & z (\texttt{zmag}), y (\texttt{ymag})  \\[2pt]
2MASS (2M)            & J (\texttt{Jmag}), H (\texttt{Hmag}), Ks (\texttt{Kmag})  \\[2pt]
CatWISE2020 (CW) & W1 (\texttt{W1mproPM}), W2 (\texttt{W2mproPM})  \\[2pt]
AllWISE (AW)     & W3 (\texttt{W3mag}), W4 (\texttt{W4mag}) \\
\hline                                   %inserts single line
\end{tabular}
}
\tablefoot{
\tablefoottext{a}{In parentheses are shown the names of the columns or features in our dataset that represent each band.}
}
\end{table}

\begin{figure}
   \centering
   \script{fig_bands_depth.py}
   \includegraphics[width=0.99\columnwidth]{figures/surveys_depth_HETDEX.pdf}
   \caption{Flux and magnitude depths ($5{-}\sigma$) from the surveys and bands used in this work. Limiting magnitudes and fluxes were obtained from the description of the surveys, as referenced in Sect.~\ref{sec:data_collection}. In purple, rest-frame SED from Mrk231 \citep[$z = 0.0422$,][]{2019MNRAS.489.3351B} is displayed as an example AGN. Redshifted (from $z {=} 0.001$ to $z {=} 7$) versions of this SED are shown in dashed grey lines.}
   \label{fig:surveys_depth_HETDEX}
\end{figure}

AGN labels and redshift information were obtained by cross-matching (with a 1.1\arcsec search radius) the catalogue with the Million Quasar Catalog\footnote{\url{http://quasars.org/milliquas.htm}} \citep[MQC, v7.4d;][]{2021arXiv210512985F}, which lists information from more than $1\,500\,000$ objects that have been classified as optical QSO, AGN, or Blazars. Sources listed in the MQC may have additional counterpart information, including radio or X-ray associations. For the purposes of this work, only sources with secure spectroscopic redshifts were used. The matching yielded $50\,538$ spectroscopically confirmed AGN in HETDEX and $17\,743$ confirmed AGN in Stripe 82.

Similarly, the sources in our parent catalogue were cross-matched with the Sloan Digital Sky Survey Data Release 16 \citep[SDSS-DR16;][]{2020ApJS..249....3A}. This cross-match was done solely to determine which sources have been spectroscopically classified as galaxies (\verb|spClass == GALAXY|). 
For most of these galaxies, SDSS-DR16 lists a spectroscopic redshift value, which will be used in some stages of this work. In the HETDEX field, SDSS-DR16 provides $68\,196$ spectroscopically confirmed galaxies. In the Stripe 82 field, SDSS-DR16 identifies $4\,085$ galaxies spectroscopically. Given that MQC has access to more AGN detection methods than SDSS, when sources were identified as both galaxies (in SDSS-DR16) and AGN (in the MQC), a final label of AGN was given. 
A description of the number of elements in each field and the multi-wavelength counterparts found for them is presented in Table~\ref{table:composition_catalogue}.

\begin{table}
\setlength{\tabcolsep}{3pt}
\caption{Composition of initial catalogue and number of cross matches with additional surveys and catalogues.}             % title of Table
\label{table:composition_catalogue}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.55\columnwidth}{!}{
\begin{tabular}{c c c}        % centered columns (3 columns)
\hline\hline                 % inserts double horizontal lines  
                &  HETDEX        &  Stripe82     \\
Survey          &                &               \\
\hline
CatWISE2020     & $15\,136\,878$ & $3\,590\,306$ \\
AllWISE         & $5\,955\,123$  & $1\,424\,576$ \\
Pan-STARRS      & $4\,837\,580$  & $1\,346\,915$ \\
2MASS           & $566\,273$     & $214\,445$    \\
LoTSS           & $187\,573$     & \ldots        \\
VLAS82          & \ldots         & $8\,747$      \\
MQC (AGN)       & $50\,538$      & $17\,743$     \\
SDSS (Galaxy)   & $68\,196$      & $4\,085$      \\
\hline
\hline                                   %inserts single line
\end{tabular}
}
\end{table}

Attending to the intrinsic differences between ML algorithms, not all of them have the same performance when being trained with features spanning a wide range of values (i.e. several orders of magnitude). For this reason, it is customary to re-scale the available values to either be contained within the range $[0, 1]$ or to have similar distributions. We applied a version of the latter transformation to our features (not the targets) as to have a mean value of $\mu = 0$ and a standard deviation of $\sigma = 1$ for each feature. Additionally, these new values were power-transformed to resemble a Gaussian distribution. This transformation helps the models avoid using the distribution of values as additional information for the training. For this work, a Yeo-Johnson transformation \citep{10.1093/biomet/87.4.954} was applied.

\subsection{Feature pool}\label{sec:feature_creation}

The initial pool of features that have been selected or engineered to use in our analysis is briefly described below:

\begin{itemize}
    \item Photometry, both measured and imputed, in the form of AB magnitudes for a total of $12$ bands.
    
    \item Colours. All available colours from measured and imputed magnitudes were considered. In total, there are $66$ colours, resulting from all available combinations of two magnitudes between the $12$ selected bands. These colours are labelled in the form \verb|X_Y| where \verb|X| and \verb|Y| are the respective magnitudes.

    \item Number of non-radio bands in which a source has valid measurements (\verb|band_num|) . This feature could be, very loosely, attributed to the total flux a source can display. A higher \verb|band_num| will imply that such source can be detected in more bands, hinting a higher flux (regardless of redshift). The use of features with counting or aggregation of elements in the studied dataset is well established in ML \citep[see, for example,][]{zheng2018feature, duboue2020art, 2021AJ....161..141S, 2022arXiv220913074H}.
    
    \item AGN-Galaxy classification boolean flag named \verb|class|.
    
    \item Radio boolean flag \verb|LOFAR_detect|. This feature flags whether sources have counterparts in the radio catalogues (LoTSS or VLAS82).
\end{itemize}

A list of the features created for this work and their representation in the code and in some of the figures is presented in Table~\ref{table:feature_names_in_work}.

%--------------------------------------------------------------------
\section{Machine Learning training}\label{sec:ML_training}

In an attempt to extract the largest available amount of information from the data, and let ML algorithms improve their predictions, we have decided to perform our training and predictions through a series of sequential steps, which we refer to as `models' henceforth. We have started with the training and prediction of the class of sources (AGN or galaxies). The next model predicts whether an AGN could be detected in radio at the depth used during training (LoTSS). A final model will predict the redshift values of radio-predicted AGN. A visual representation of this process can be seen in Fig.~\ref{fig:pipeline_flowchart}. Creating separate models gives us the opportunity to select the best subset of features for training as well as the best combination of ML algorithms for training in each step.

\begin{figure}
   \centering
   \script{fig_flowchart_pipeline.py}
   \includegraphics[width=0.51\columnwidth]{figures/flowchart_pipeline_radio_AGN_z.pdf}
   \caption{Flowchart representing the prediction pipeline used in Stripe 82 to predict the presence of radio-detectable AGN and their redshift values. At the beginning of each model step, the most relevant features are selected as described in Sect.~\ref{sec:feat_selection}.}
   \label{fig:pipeline_flowchart}
\end{figure}

In broad terms, our goal with the classification models is to recover the largest number of elements from the positive classes (\texttt{class = 1} and \texttt{LOFAR$\_$detect = 1}). For the regression model, we aim to retrieve predictions as close as the originally fed redshift values.

In general, classification models provide a final score in the range [$0$, $1$], which can only be associated with a true probability after a careful calibration  \citep{10.1214/17-EJS1338SI, pmlr-v54-kull17a}. Calibration of these scores can be done by applying a transformation to their values. For our work, we will apply a Beta transformation\footnote{Beta transformation functions have the general form $\mu_{beta}(S;a,b,c) = 1/\left(1 + 1 / \left(e^{c} \frac{S^{a}}{(1 - S)^{b}}\right)\right)$, with $S$ being the score from the classifier and $a,b,c$, free parameters to be optimised.}. This type of transformation allows to re-distribute the scores of a classifier allowing them to get closer to the definition of probability. Further details of this calibration are given in the Appendix~\ref{app:calibration_models}.

Given that we need to be able to compare the results from the training and application of the ML models with values obtained independently (i.e. ground truth), we divided our dataset into labelled and unlabelled sources. Labelled sources are all elements of our catalogue that have been classified as either AGN or galaxies. Unlabelled sources are those which lack such classification and that will be subject to the prediction of our models.  

Before any calculation or transformation is applied to the data from the HETDEX field, we split the labelled dataset into training, validation, calibration, and testing subsets. The early creation of these subsets helps avoid information leakage from the test subset into the models. Initially, a $20 \%$ of the dataset has been reserved as testing data. Of the remaining elements, an $80 \%$ of them have been used for training, and the rest of the data has been divided equally between calibration and validation subsets (i.e. a $10 \%$ each). The splitting process and the number of elements for each subset are shown in Fig.~\ref{fig:dataset_sizes}. Depending on the model, the needed sources are selected from each of the sub-sets that have been already created. The use of these subsets will be shown in Sects.~\ref{sec:model_selection} and \ref{sec:models_training}. 

\begin{figure}
  \centering
  \begin{minipage}{0.65\columnwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/flowchart_HETDEX_subsets.pdf}\hfill\break
    {(a) HETDEX Field}
  \end{minipage}
  \hfill
  \begin{minipage}{0.34\columnwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/flowchart_S82_subsets.pdf}
    \vspace{3.6cm}\hfill\break
    {(b) Stripe 82 Field}
  \end{minipage}
  \caption{Composition of datasets used for the different steps of this work. (a) HETDEX Field. (b) Stripe 82.}
  \label{fig:dataset_sizes}
\end{figure}

All the following transformations (feature selection, standardisation, and power transform of features) have been applied to the training and validation subsets before the training of the algorithms and models. 
The calibration and testing subsets were subject to the same transformations after the modelling stage.

\subsection{Feature selection}\label{sec:feat_selection}

ML algorithms, as with most data analysis tools, require execution times which increase at least linearly with the size of the datasets. In order to reduce training times without losing relevant information for the model, the most important features were selected at each step through a process called feature selection. 

To avoid redundancy, the process starts discarding features that have a high correlation with another property of the dataset. For discarding features, we calculated Pearson's correlation matrix for the full train+validation dataset only and selected the pairs of features that showed a correlation factor higher than $\rho = 0.75$, in absolute values\footnote{A value of $\rho = 0.75$ is a compromise between very stringent thresholds (e.g. $\rho = 0.5$) and more relaxed values (e.g. $\rho \approx 0.9$). For an explanation on how to consider different correlation values, see, for instance \citet{Ratner2009}}. From each pair, we discarded the feature with the lowest relative standard deviation \citep[RSD;][]{johnson1964statistics}. The RSD is defined as the ratio between the standard deviation of a set and its mean value. A feature which covers a small portion of its probable values (i.e. low coverage of parameter space, and lower RSD) will give less information to a model than one with largely spread values.

%After this, we repeat this procedure but using the Predictive Power Score \citep[PPS, \texttt{v1.2.0};][]{PPSsoftware}. PPS helps reveal non-linear or non-monotonic patterns among features which might not be unveiled by traditional correlation factors. PPS achieves this by creating two models (a decision tree) per pair of columns in the dataset. One model has one of the features of the pair as training data and the other feature as the target. The second model does the same but inverts the order of the features. The comparison of the results from such models with a naive prediction (always predicting the median of the target or the most common class) lead to the assessment of the predicting power of each feature. As with Pearson's correlation, and to keep a consistent behaviour between both methods, if $\mathrm{PPS} > 0.75$ for a model, the target feature can be discarded from the dataset.

%A third and final step was applied to the data for optimising the final number of used features. The Boruta method \citep{JSSv036i11} and its Python implementation\footnote{\texttt{v0.3}; \url{https://github.com/scikit-learn-contrib/boruta_py}} were developed to discard features that do not add extra information to the training of a ML model. Boruta creates additional features with randomly-sorted versions of the quantities present in a dataset. Then, it runs a simple training procedure and calculates which feature is more helpful to the training: the randomised or the original version. If the original feature performs worse than its shuffled copy, it can be safely discarded from the dataset reducing its size without a major loss of relevant information.

For each model, the process of feature selection begins with $79$ base features and three targets (\verb|class|, \verb|LOFAR_detect|, and $z$). Feature selection is run, independently, for each trained model (i.e. AGN-Galaxy classification, radio detection, and redshift predictions), delivering three different sets of features.

\subsection{Metrics}\label{sec:metrics}

A set of metrics will be used to understand the reliability of the results and put them in context with results in the literature. 
Since our work includes the use of classification and regression models, we briefly discuss the appropriate metrics in the following sections.

\subsubsection{Classification metrics}\label{sec:metrics_classfication}

The main tool to assess the performance of classification methods is the Confusion (or Error) Matrix. It is a two-dimension (predicted vs. true) matrix where the true and predicted class(es) are compared and results stored in cells with the rate of True Positives (TP), True Negative (TN), False Positives (FP), and False Negatives (FN). As mentioned earlier in Sect.~\ref{sec:ML_training}, we seek to maximise the number of positive-class sources that are recovered as such. Using the elements of the confusion matrix, this aim can be translated into the maximisation of TP and, consequently, the minimisation of FN.

From the elements of the confusion matrix, we can obtain additional metrics, such as the F1 and $\mathrm{F}_{\beta}$ scores \citep{10.2307/1932409, sorenson1948method, van1979information}, and the Matthews Correlation Coefficient \citep[MCC;][]{10.2307/2340126, nla.cat-vn81100, MATTHEWS1975442} which are better suited for unbalanced data as they take into account the behaviour and correlations among all elements of the confusion matrix.
As such, the F1 coefficient is defined as: 

\begin{equation}\label{eq:f1}
\mathrm{F1} = \frac{2 \mathrm{TP}}{2 \mathrm{TP} + \mathrm{FN} + \mathrm{FP}}\,.
\end{equation}

\noindent 
F1 values can go from $0$ (no prediction of positive instances) to $1$ (perfect prediction of elements with positive labels). This definition assigns equal weight (importance) to both the number of FN and FP. An extension to the F1 score, which adds a non-negative parameter, $\beta$, to increase the importance given to each one of them is the F-Score ($\mathrm{F}_{\beta}$), defined as:

\begin{equation}\label{eq:f_beta}
\mathrm{F}_{\beta} = \frac{(1 + \beta^{2}) \times \mathrm{TP}}{(1 + \beta^{2}) \times \mathrm{TP} + \beta^{2} \times \mathrm{FN} + \mathrm{FP}}\,.
\end{equation}

Using ${\beta > 1}$, more relevance is given to the optimisation of FN. When ${0 \leq \beta < 1}$, the optimisation of FP is more relevant. If $\beta = 1$, the initial definition of F1 is recovered. As with F1, $\mathrm{F}_{\beta}$ values can be in the range ${[0, 1]}$. As we seek to minimise the number of FN detection, we adopt a conservative value of ${\beta = 1.1}$, giving more significance to their reduction without removing the aim for FP. Also, this value is close enough to $\beta = 1$, which will allow us to compare our scores to those produced in previous works.

MCC is defined as:

\begin{equation}\label{eq:mcc}
\mathrm{MCC} = \frac{\mathrm{TP} \times \mathrm{TN} - \mathrm{FP} \times \mathrm{FN}}{\sqrt{(\mathrm{TP} + \mathrm{FP}) (\mathrm{TP} + \mathrm{FN}) (\mathrm{TN} + \mathrm{FP}) (\mathrm{TN} + \mathrm{FN})}}\,,
\end{equation}

\noindent which includes also the information about the TN elements. MCC can range from $-1$ (total disagreement between true and predicted values) to $+1$ (perfect prediction) with $0$ representing a prediction analogous to a random guess.

The Recall \citep[also called Completeness, Sensitivity, or True Positive Rate -TPR-;][]{10.2307/4586294} corresponds to the rate of relevant, or correct, elements that have been recovered by a process. Using the elements from the confusion matrix, it can be defined as:

\begin{equation}\label{eq:recall}
\mathrm{Recall} = \mathrm{TPR} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}.\,
\end{equation}

The TPR can go from $0$ to $1$, with a value of $1$ meaning that the model can recover all the true instances. 

The last metric used is Precision (also known as Purity), which can be defined as the ratio between the number of correctly classified elements and the number of sources in the positive class (AGN or radio detectable): 

\begin{equation}\label{eq:precision}
\mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}\,.
\end{equation}

Precision can range from $0$ to $1$ where higher values show that more real positive instances of the studied set were retrieved as such by the model.

\subsubsection{Regression metrics}\label{sec:metrics_regression}

For the case of individual redshift value determination, two commonly used metrics are the difference between predicted and true redshift,

\begin{equation}
\Delta z = z_{\mathrm{True}} - z_{\mathrm{Predicted}}\,,
\end{equation}

\noindent and its normalised difference,

\begin{equation}\label{eq:delta_z_N}
\Delta z^{\mathrm{N}} = \frac{z_{\mathrm{True}} - z_{\mathrm{Predicted}}}{1 + z_{\mathrm{True}}}\,.
\end{equation}

If the comparison is made over a larger sample of elements, the bias of the redshift is used \citep{2013ApJ...775...93D}, with the median of the quantities instead of its mean to avoid the strong influence of extreme values:

\begin{eqnarray}
\Delta z_{\mathrm{Total}} &=& \mathrm{median}\left(z_{\mathrm{True}} - z_{\mathrm{Predicted}}\right) = \mathrm{median}(\Delta z)\,,\\
\Delta z_{\mathrm{Total}}^{\mathrm{N}} &=& \mathrm{median}\left(\frac{z_{\mathrm{True}} - z_{\mathrm{Predicted}}}{1 + z_{\mathrm{True}}}\right) = \mathrm{median}(\Delta z^{\mathrm{N}})\,.
\end{eqnarray}

Using the previous definitions, four additional metrics can be calculated. These are the median absolute deviation (MAD, $\sigma_{\mathrm{MAD}}$) and normalised median absolute deviation \citep[NMAD, $\sigma_{\mathrm{NMAD}}$;][]{hoaglin1983understanding, 2009ApJ...690.1236I}, which are less sensitive to outliers. Also, the standard deviation of the predictions, $\sigma_{z}$, and its normalised version, $\sigma_{z}^{\mathrm{N}}$ are typically used. They are defined as:

\begin{eqnarray}
\sigma_{\mathrm{MAD}} &=& 1.48 \times \mathrm{median}\left(|\Delta z|\right)\,,\\
\sigma_{\mathrm{NMAD}} &=& 1.48 \times \mathrm{median}\left(\left|\Delta z^{\mathrm{N}}\right|\right)\,,\\
\sigma_{z} &=& \sqrt{\frac{1}{\mathrm{d}} \sum_{i}^{\mathrm{d}} \left(\Delta z\right)^{2}}\,,\\
\sigma_{z}^{\mathrm{N}} &=& \sqrt{\frac{1}{\mathrm{d}} \sum_{i}^{\mathrm{d}} \left(\Delta z^{\mathrm{N}}\right)^{2}}\,,
\end{eqnarray}

\noindent with d being the number of elements in the studied sample (i.e. its size).

Also, the outlier fraction \citep[$\eta$, as used in][]{2013ApJ...775...93D, 2022A&C....3800510L} is considered, which is defined as the fraction sources with a predicted redshift difference ($\left|\Delta z^{\mathrm{N}}\right|$, Eq.~\ref{eq:delta_z_N}) larger than a previously set value. Taking the results from \citet{2009ApJ...690.1236I} and \citet{2010A&A...523A..31H}, we have selected this threshold to be $0.15$, leaving the definition of the outlier fraction as:

\begin{equation}\label{eq:outlier_fraction}
\eta = \frac{\# \left( \left|\Delta z^{\mathrm{N}}\right| > 0.15 \right)}{d}\,.
\end{equation}

\noindent where $\#$ symbolises the number of sources fulfilling the described relation, and $d$ corresponds to the size of the selected sample.

%For the selection of algorithms and comparison of results in this work, the main metric used in this work are $\sigma_{\mathrm{MAD}}$ and $\sigma_{\mathrm{NMAD}}$.

\subsubsection{Calibration metrics}\label{sec:metrics_calibration}

One of the most used analytical metrics to assess calibration of a model is the Brier score \citep[BS;][]{Brier_1950}. It measures the mean square difference between the predicted probability of an element and its true class. If the total number of elements in the studied sample is $d$, the BS can be written (for binary classification problems, as the ones studied in this work) as:

\begin{equation}\label{eq:brier_score}
\mathrm{BS} = \frac{1}{d} \sum_{i}^{d}(p - \mathrm{class})^{2}\,,
\end{equation}

\noindent where $p$ is the predicted class and \textrm{class} the true class of each of the elements in the sample ($0$ or $1$).
The BS can range between $0$ and $1$ with $0$ representing a model that is completely reliable in its predictions.

Additionally, the BS can be used to compare the reliability (or calibration) between a model and a reference using the Brier Skill Score \citep[BSS; e.g.][]{Glahn_1970}:

\begin{equation}\label{eq:brier_skill_score}
\mathrm{BSS} = 1 - \frac{\mathrm{BS}}{\mathrm{BS}_{\mathrm{ref}}}\,.
\end{equation}

In our case, $\mathrm{BS}_{\mathrm{ref}}$ corresponds to the value calculated from the uncalibrated model. The BSS can take values between $-1$ and $+1$. The closer the BSS gets to $1$, the more reliable the analysed model is. These values include the case where ${\mathrm{BSS} {\approx} 0}$, in which both models perform similarly in terms of calibration.

For our pipeline, after a model has been fully trained, a calibrated version of their scores will be obtained. With both of them, the BSS will be calculated and, if it is not much lower than $0$, that calibrated transformation will be used as the final scores from the prediction.

\subsection{Model selection}\label{sec:model_selection}

By design, each ML algorithm has been developed and tuned to work better with certain data conditions, i.e. balance of target categories, ranges of base features, etc. 
The predicting power of different algorithms can be combined with the use of meta-learners \citep{Vanschoren2019}. Meta-learners use the properties or predictions from other algorithms (base learners) as additional information during their training stages. A simple implementation of this procedure is called Generalised Stacking \citep{WOLPERT1992241} which can be interpreted
as the addition of priors to the model training stage. Generalised stacking has been applied in several astrophysical problems. That is the case of \citet{2016MNRAS.460.3152Z}, \citet{2022arXiv220913074H}, \citet{2022A&A...666A..87C}, and \citet{2022arXiv220614944E}.

Base and meta learners have been selected based upon the metrics described in Sect.~\ref{sec:metrics}. We have trained five algorithms with the training subset and calculated the metrics for all of them using a $10$-fold cross-validation approach \citep[e.g.][]{https://doi.org/10.1111/j.2517-6161.1974.tb00994.x, doi:10.1080/00401706.1974.10489157} over the same training subset. For each metric, the learners have been given a rank (from $1$ to $5$) and a mean value has been obtained from them. Out of the analysed algorithms, the one with the best overall performance (i.e. best mean rank) is selected to be the meta learner while the remaining four are used as base learners.

For the AGN-galaxy classification and radio detection problems, we tested five classification algorithms: Random Forest \citep[\texttt{RF};][]{Breiman2001}, Gradient Boosting Classifier \citep[\texttt{GBC};][]{10.1214/aos/1013203451}, Extra Trees \citep[\texttt{ET};][]{Geurts2006}, Extreme Gradient Boosting \citep[\texttt{XGBoost}, \texttt{v1.5.1};][]{Chen:2016:XST:2939672.2939785}, and \texttt{CatBoost} \citep[\texttt{v1.0.5};][]{DBLP:journals/corr/DorogushGGKPV17, DBLP:journals/corr/abs-1810-11363}.
For the redshift prediction problem, we tested five regressors as well: \texttt{RF}, \texttt{ET}, \texttt{XGBoost}, \texttt{CatBoost}, and Gradient Boosting Regressor \citep[\texttt{GBR};][]{10.1214/aos/1013203451}.
We have used the Python implementations of these algorithms and, in particular for \texttt{RF}, \texttt{ET}, \texttt{GBC}, and \texttt{GBR}, the versions offered by the package \texttt{scikit-learn}\footnote{\url{https://scikit-learn.org}} \citep[\texttt{v0.23.2};][]{scikit-learn}. These algorithms were selected given that they offer tools to interpret the global and local influence of the input features in the training and predictions (cf. Sect.~\ref{sec:introduction} and \ref{sec:model_explain}).

All the algorithms selected for this work fall into the broad family of Tree-Based models. Forest models (\texttt{RF} and \texttt{ET}) rely on a collection of decision trees to, after applying a majority vote, predict either a class or a continuum value. Each of these decision trees uses a different, randomly-selected sub-set of features to make a decision on the training set \citep{Breiman2001}. Opposite to forests, Gradient Boosting models (\texttt{GBC}, \texttt{GBR}, \texttt{XGBoost} and \texttt{CatBoost}) apply decision trees sequentially to improve the quality of the previous predictions \citep{10.1214/aos/1013203451, FRIEDMAN2002367}.

\subsection{Training of models}\label{sec:models_training}

The procedure described in Sect.~\ref{sec:model_selection} includes an initial fit of the selected algorithms to the training data (including the selected features) to optimise their parameters. The stacking step includes a new optimisation of the parameters of the meta-learner using $10$-fold cross-validation on the training data with the addition of the output from the base learners, which are treated as regular features. Then, the hyper-parameters of the stacked models are optimised over the training sub-set (a brief description of this step is presented in Appendix~\ref{sec:app_hyperpars}).

The final step involves a last parameter fitting instance but using, this time, the combined train+validation subset, which includes the output of the base algorithms, to ensure wider coverage of the parameter space and better-performing models. Consequently, only the testing set is available for assessing the quality of the predictions made by the models.

\subsection{Probability calibration}\label{sec:prob_calibration}

The calibration procedure was performed in the calibration subset. In this way, we avoid influencing the process with information from the training and validation steps. A broader description of the calibration process and the results obtained for our models are presented in Appendix~\ref{app:calibration_models}.

From this point onward, and with the sole exception of some of the outcomes shown in Sect.~\ref{sec:model_explain}, all results from classifications will be based on the calibrated probabilities.

\subsection{Optimisation of classification thresholds}\label{sec:threshold_opt}

As mentioned in the first paragraphs of Sect.~\ref{sec:ML_training}, classification models deliver a range of probabilities for which a threshold is needed to separate their predictions between negative and positive classes. By default, these models set a threshold at $0.5$ in score\footnote{Throughout this work, we will call this a naive threshold.} but, in principle, and given the characteristics of the problem, a different optimal threshold might be needed.

In our case, we want to optimise (increase) the number of recovered elements in each model (i.e. AGN or radio-detectable sources). This maximisation corresponds to obtaining thresholds that optimise the recall given a specific precision limit. We did that with the use of the statistical tool called Precision-Recall (PR) Curve. A deeper description of this method and the results obtained from our work are presented in Appendix~\ref{sec:app_pr_curve}\footnote{Thresholds derived from the PR curves will be labelled as PR.}.

%----------------------------------------------------------------

\section{Results}\label{sec:results}
In the present section, we report the results from the training of the different models in the HETDEX field. All metrics are evaluated using the testing subset. The metrics are also computed on labelled AGN in the Stripe 82 field. As no training is done on Stripe\,82 data, it offers a way to test the validity of the pipeline on data that, despite having similar optical-NIR photometric properties, presents distinct radio information and location in the sky.

The three models are chained afterwards in sequential mode to create a pipeline, and related metrics, for the prediction of radio-AGN activity. Novel predictions were obtained from the application of such pipeline to unlabelled sources from both the HETDEX and Stripe\,82 fields. 

\subsection{AGN-Galaxy classification}\label{sec:results_agn}
Feature selection was applied to the train+validation subset with $85\,488$ confirmed elements (galaxies from SDSS DR16 and AGN from MQC, i.e. \texttt{class == 0} or \texttt{class == 1}). After the selection procedure described in Sect.~\ref{sec:feat_selection}, $18$ features were selected for training: \verb|band\_num|, \verb|W4mag|, \verb|g_r|, \verb|r_i|, \verb|r_J|, \verb|i_z|, \verb|i_y|, \verb|z_y|, \verb|z_W2|, \verb|y_J|, \verb|y_W1|, \verb|y_W2|, \verb|J_H|, \verb|H_K|, \verb|H_W3|, \verb|W1_W2|, \verb|W1_W3|, and \verb|W3_W4|. The target feature is \verb|class|.

The results of model testing for the AGN-galaxy classification are reported in Table~\ref{table:fit_AGN_models}. The \verb|CatBoost| algorithm provides the best metric values (highest mean rank) and is therefore selected as the  meta-model. \verb|XGBoost|, \verb|RF|, \verb|ET|, and \verb|GBC| were used as base learners.

\begin{table}
  \caption{Best performing models for the AGN-galaxy classification}             % title of Table
  \label{table:fit_AGN_models}      % is used to refer this table in the text
  \centering                          % used for centering table
  \resizebox{0.88\columnwidth}{!}{
  \begin{tabular}{c c c c c c}        % centered columns (6 columns)
  \hline\hline                 % inserts double horizontal lines   
  Model             & F$_{\beta}$ & MCC    & Precision  & Recall & Mean rank \\
  \hline
  \texttt{CatBoost} & 0.9570      & 0.9246 & 0.9545     & 0.9591 & 1.00      \\
  \texttt{XGBoost}  & 0.9567      & 0.9240 & 0.9541     & 0.9588 & 2.00      \\
  \texttt{RF}       & 0.9552      & 0.9214 & 0.9528     & 0.9571 & 3.00      \\
  \texttt{ET}       & 0.9540      & 0.9194 & 0.9513     & 0.9563 & 4.00      \\
  \texttt{GBC}      & 0.9526      & 0.9166 & 0.9482     & 0.9563 & 5.00      \\
  \hline                                   %inserts single line
  \end{tabular}
  }
  \tablefoot{Metrics obtained using the default probability threshold of $0.5$.
  }
  \end{table}

The optimisation of the PR curve for the calibrated predictor provides an optimal threshold for this algorithm of $0.34895$. This value was used for the AGN-Galaxy model throughout this work.

The results of the application of the stacked and calibrated model for the testing subset and the labelled sources in S82 are presented in Table~\ref{table:fit_AGN_results}. The metrics are shown for the use of two different thresholds, the naive value of $0.5$ and the  PR-derived value of $0.34895$. The confusion matrix (calculated on the testing dataset) is shown in the upper left panel of Fig.~\ref{fig:results_models_test}. 

%The scores for the three subsets are very similar with no substantial over-fitting tracked by the testing set score.
Overall, the model is able to separate AGN from galaxies with a very high (recall ${\gtrsim} 94\%$) success rate.

\begin{table}
\setlength{\tabcolsep}{3pt}
\caption{Resulting metrics of AGN-galaxy classification model for the test subset and the labelled sources in S82 using two different threshold values, as described in Sect.~\ref{sec:results_agn}. HETDEX and Stripe 82 pipeline results are described in Sect.~\ref{sec:results_prediction_pipeline}.}             % title of Table
\label{table:fit_AGN_results}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.90\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
Subset                            & Threshold & F$_{\beta}$ & MCC     & Precision & Recall \\
\hline
\multirow{2}{*}{HETDEX-test} & Naive     & 0.9537      & 0.9181  & 0.9747    & 0.9589 \\
                                  & PR        & 0.9542      & 0.9185  & 0.9449    & 0.9621 \\
\multirow{2}{*}{S82-labelled}  & Naive     & 0.9415      & 0.7054  & 0.9516    & 0.9333 \\
                                  & PR        & 0.9437      & 0.7067  & 0.9481    & 0.9401 \\
\multirow{2}{*}{HETDEX-pipeline}  & Naive     & 0.9537      & 0.9181  & 0.9747    & 0.9589 \\
                                  & PR        & 0.9542      & 0.9185  & 0.9449    & 0.9621 \\
\multirow{2}{*}{S82-pipeline}     & Naive     & 0.9415      & 0.7054  & 0.9516    & 0.9333 \\
                                  & PR        & 0.9437      & 0.7067  & 0.9481    & 0.9401 \\
\hline                                   %inserts single line
\end{tabular}
}
\end{table}

\begin{figure}
  \centering
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.97\textwidth]{figures/conf_matrix_AGN_HETDEX_test.pdf}\hfill\break
    {(a) AGN-galaxy classification}
  \end{minipage}%\\%
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/conf_matrix_radio_HETDEX_test.pdf}\hfill\break
    {(b) Radio detection on AGN}
  \end{minipage}\hfill\break%\\%
  \begin{minipage}{0.70\columnwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/compare_redshift_HETDEX_test.pdf}\hfill\break
    {(c) Redshift on radio AGN}
  \end{minipage}%
  \caption{Performance of individual models (AGN-Galaxy classification, radio-detectability classification and redshift regression) when applied to the HETDEX test subset. (a): confusion matrix for AGN-galaxy classification. 
  (b): Same as (a), but for radio detection. (c): Density plot comparison between original and the predicted redshifts. 
  The grey, dashed line shows the 1:1 relation while dot-dashed lines show the limits for outliers (cf. Eq.~\ref{eq:outlier_fraction}). Inset displays the distribution of $\Delta z^{\mathrm{N}}$ with a ${{<}\Delta z^{\mathrm{N}}{>} = 0.0442}$.}
  \label{fig:results_models_test}
\end{figure}

\subsection{Radio detection}\label{sec:results_radio}

Training of the radio detection model was applied only to sources confirmed to be AGN (\texttt{class == 1}).
Feature selection was applied to the train+validation subset, with $36\,387$ confirmed AGN. 
The target feature is \verb|LOFAR_detect| and the base of selected features are:  \verb|band_num|, \verb|W4mag|, \verb|g_r|, \verb|g_i|, \verb|r_i|, \verb|r_z|, \verb|i_z|, \verb|z_y|, \verb|z_W1|, \verb|y_J|, \verb|y_W1|, \verb|J_H|, \verb|H_K|, \verb|K_W3|, \verb|K_W4|, \verb|W1_W2|, and \verb|W2_W3|.

The performance of the tested algorithms is shown in Table~\ref{table:fit_radio_models}. 
In this case, \verb|GBC| shows the highest mean rank. For this reason, we used it as the meta-learner and \verb|XGBoost|, \verb|CatBoost|, \verb|RF|, and \verb|ET| were selected as base-learners.

% The scores from the stacked classifier are fully concentrated at around $0.5$. This might be interpreted as a sign that the model is not able to classify with strong certainty the studied sources.

The optimal threshold for this model is found to be ${\sim}0.20460$.% with mild effect on the metrics when compared to the naive classification.

\begin{table}
\caption{Best performing models the radio detection classification}             % title of Table
\label{table:fit_radio_models}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.88\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
Model               & F$_{\beta}$   & MCC    & Precision   & Recall & Mean rank \\
\hline
\texttt{XGBoost}    & 0.2998        & 0.2981 & 0.5674      & 0.2161 & 2.75      \\
\texttt{CatBoost}   & 0.2957        & 0.3056 & 0.6010      & 0.2085 & 2.25      \\
\texttt{GBC}        & 0.2960        & 0.3131 & 0.6255      & 0.2066 & 1.75      \\
\texttt{RF}         & 0.2916        & 0.3026 & 0.6003      & 0.2048 & 3.75      \\
\texttt{ET}         & 0.2840        & 0.2973 & 0.6006      & 0.1980 & 4.50      \\
\hline                                   %inserts single line
\end{tabular}
}
\tablefoot{Metrics obtained using the default probability threshold of $0.5$.\\
Algorithms are sorted by decreasing recall values.}
\end{table}

\begin{table}
\setlength{\tabcolsep}{3pt}
\caption{Resulting metrics of the radio detection model on the test subset and the labelled sources in S82 using two different threshold values, as explained in Sect.~\ref{sec:results_radio}. HETDEX and Stripe 82 pipeline results shown as part of the discussion in Sect.~\ref{sec:results_prediction_pipeline}.}             % title of Table
\label{table:fit_radio_results}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.90\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
Subset                             & Threshold & F$_{\beta}$ & MCC  & Precision & Recall  \\
\hline
\multirow{2}{*}{HETDEX-test} & Naive     & 0.2487    & 0.2736 & 0.6061   & 0.1672  \\
                                    & PR        & 0.4288    & 0.3247 & 0.3528   & 0.5216  \\
\multirow{2}{*}{S82-labelled}  & Naive     & 0.2715    & 0.2336 & 0.2572   & 0.2847  \\
                                    & PR        & 0.2162    & 0.1937 & 0.1229   & 0.5816  \\
\multirow{2}{*}{HETDEX-pipeline}    & Naive     & 0.2437    & 0.2693 & 0.5936   & 0.1638  \\
                                    & PR        & 0.4157    & 0.3167 & 0.3465   & 0.4980  \\
\multirow{2}{*}{S82-pipeline}       & Naive     & 0.2652    & 0.2329 & 0.2571   & 0.2772  \\
                                    & PR        & 0.2019    & 0.1840 & 0.1145   & 0.5478  \\
\hline                                   %inserts single line
\end{tabular}
}
\end{table}

Finally, the stacked model metrics and confusion matrix are shown in Table~\ref{table:fit_radio_results}, for PR-optimised and naive thresholds, and in  Fig.~\ref{fig:results_models_test} respectively. 
%In this case, the metrics of the testing subset do show a somewhat noticeable deterioration from those in the training and validation subsets. This implies that the trained model is not able to extrapolate the information it was given in order to predict the behaviour of additional data.  Nonetheless, the confusion matrix shows a high success of radio emission prediction on AGN (recall ${\sim} 65 \%$).

\subsection{Redshift predictions}\label{sec:results_redshift}

The redshift value prediction model was applied to sources confirmed to be radio-detected AGN (i.e. \texttt{class~==~1} and \texttt{radio\_detect~==~1}).
Feature selection (cf. Sect.~\ref{sec:feat_selection}) was applied to the train+validation subset, with $4\,612$ sources, leading to the selection of $17$ features. The target feature is \verb|Z| and the selected base features are \verb|band_num|, \verb|W4mag|, \verb|g_r|, \verb|g_W3|, \verb|r_i|, \verb|r_z|, \verb|i_z|, \verb|i_y|, \verb|z_y|, \verb|y_J|, \verb|y_W1|, \verb|J_H|, \verb|H_K|, \verb|K_W3|, \verb|K_W4|, \verb|W1_W2|, and \verb|W2_W3|.
\\
\begin{table}
\setlength{\tabcolsep}{3pt}
\caption{Results of initial fit for redshift value prediction}             % title of Table
\label{table:fit_redshift_models}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{c c c c c c c}        % centered columns (7 columns)
\hline\hline                 % inserts double horizontal lines   
Model               & $\sigma_{\mathrm{MAD}}$   & $\sigma_{\mathrm{NMAD}}$  & $\sigma_{z}$  & $\sigma_{z}^{\mathrm{N}}$ & $\eta$ & Mean rank \\
\hline
\texttt{RF}         & 0.1788                    & 0.0795                    & 0.4202        & 0.1938                    & 0.1951 & 2.0       \\
\texttt{ET}         & 0.1853                    & 0.0842                    & 0.4112        & 0.1865                    & 0.1924 & 1.8       \\
\texttt{CatBoost}   & 0.2171                    & 0.1008                    & 0.4035        & 0.1852                    & 0.2193 & 2.2       \\
\texttt{XGBoost}    & 0.2289                    & 0.1084                    & 0.4314        & 0.1962                    & 0.2415 & 4.0       \\
\texttt{GBR}        & 0.2773                    & 0.1272                    & 0.4482        & 0.2041                    & 0.2867 & 5.0       \\
\hline                                   %inserts single line
\end{tabular}
}
\tablefoot{Algorithms sorted by increasing $\sigma_{\mathrm{MAD}}$ values.}
\end{table}

For the redshift prediction, the tested algorithms performed as shown in Table~\ref{table:fit_redshift_models}. 
Based on their mean rank values, \verb|RF|, \verb|CatBoost|, \verb|XGBoost|, and \verb|GBR| were selected as base learners and \verb|ET| (which shows the best $\sigma_{\mathrm{MAD}}$ value of the two models with the best rank) was used as meta-learner.
The redshift regression metrics of the stacked model are presented in Table~\ref{table:fit_redshift_results}. 
Likewise, the comparison between the original and predicted redshifts is shown in the lower panel of Fig.~\ref{fig:results_models_test}.

\begin{table}
\setlength{\tabcolsep}{3pt}
\caption{Redshift prediction metrics for the test subset from HETDEX and Stripe 82 labelled sources as discussed in Sect.~\ref{sec:results_prediction_pipeline}.}             % title of Table
\label{table:fit_redshift_results}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.93\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
Subset                 & $\sigma_{\mathrm{MAD}}$ & $\sigma_{\mathrm{NMAD}}$ & $\sigma_{z}$ & $\sigma_{z}^{\mathrm{N}}$ & $\eta$ \\
\hline
HETDEX-test             & 0.1654    & 0.0727     & 0.4114  & 0.2056       & 0.1903 \\
S82-labelled            & 0.1866    & 0.0928     & 0.5108  & 0.2469       & 0.2429 \\
HETDEX-pipeline (Naive) & 0.0996    & 0.0626     & 0.3399  & 0.2075       & 0.1897 \\
HETDEX-pipeline (PR)    & 0.1586    & 0.0717     & 0.3780  & 0.2293       & 0.1891 \\
S82-pipeline (Naive)    & 0.1558    & 0.0914     & 0.4460  & 0.2729       & 0.2484 \\
S82-pipeline (PR)       & 0.2071    & 0.0984     & 0.4514  & 0.2614       & 0.2518 \\
\hline                                   %inserts single line
\end{tabular}
}
\end{table}

%The results in Table~\ref{table:fit_redshift_results} show a slight degree of over-fitting as pointed out by the larger values for the testing scores with respect to the training subset. As in the previous classification model, the reasons could be several, including some degeneracy introduced by the imputed upper limits. Overall, the predictions of the model follow very well the observed data, with a precision, bias and outlier fraction comparable with the best models in the literature.

\subsection{Prediction pipeline}\label{sec:results_prediction_pipeline}

The sequential combination of the models described in Sect.~\ref{sec:ML_training} defines the pipeline for the prediction of radio-detectable AGN and their redshift. As separate tasks, the pipeline was applied to the labelled sources in the HETDEX testing subset, to the labelled sources in Stripe\,82, and to all unlabelled sources across both fields. Stripe\,82 provides an independent test of the pipeline as no data in this field was used for training the different models. A full candidate catalogue is extracted from this exercise and based on the unlabelled datasets.

As the metrics discussed in the previous sections correspond to each individual model, new --combined-- metrics, based on the knowledge for labelled sources, are calculated for HETDEX and Stripe\,82 and presented in Fig.~\ref{fig:conf_matx_results_radio_AGN} and Tables~\ref{table:fit_redshift_results} and \ref{table:fit_radio_AGN_results}. Overall, we observe slightly worse combined metrics  with respect to the ones calculated for individual models. This degradation might be understood by the fact that the pipeline is composed of three sequential models. Each additional step is fed with sources classified by the previous algorithm. And some of these sources might not be similar, in terms of features, to those used for training, thus adding noise to the output of such model. A small sample of the output of the pipeline for five high-$z$ labelled radio AGN sources in HETDEX and Stripe\,82 are shown in Tables~\ref{table:pred_radio_AGN_known_HETDEX} and \ref{table:pred_radio_AGN_known_S82} respectively.

The application of the prediction pipeline to the unlabelled sources from the HETDEX field led to $9\,974\,990$ predicted AGN, from which $68\,252$ were predicted to be radio detectable. The pipeline predicts, as well, $2\,073\,997$ AGN in the unlabelled data from Stripe 82, being $22\,445$ of them candidates to be detected in the radio (to the detection level of LoTSS). 
The distribution of the predicted redshifts for radio-AGN in HETDEX and Stripe 82 is presented in  Fig.~\ref{fig:hist_pred_z_unlabel_true_z_label}. The pipeline outputs for a small sample of the predicted radio AGN are presented in Tables~\ref{table:pred_radio_AGN_unknown_HETDEX} and \ref{table:pred_radio_AGN_unknown_S82} for HETDEX and Stripe\, 82 respectively.

\begin{figure}
  \centering
    \script{fig_hist_redshift_radio_AGN.py}
    \includegraphics[width=0.99\columnwidth]{figures/hist_pred_true_z_HETDEX_all_S82_nonimputed.pdf}
  \caption{Redshift density distribution of the predicted radio-AGN within the unlabelled sources (clean histograms) in HETDEX (ochre histograms) and Stripe 82 (blue histograms) and true redshifts from labelled radio-AGN (dashed histograms).}
  \label{fig:hist_pred_z_unlabel_true_z_label}
\end{figure}

Section~\ref{sec:discussion} explores the comparison of these results with previous works in the literature and discusses the main drivers (i.e. features) for the detection of these radio-AGN.

\begin{figure*}
  \centering
  \begin{minipage}{0.20\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/conf_matrix_rAGN_HETDEX_test.pdf}\hfill\break
    {(a) Radio-AGN confusion matrix}
  \end{minipage}%\\%
      \centering
  \begin{minipage}{0.30\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/compare_redshift_rAGN_HETDEX_test.pdf}\hfill\break
    {(b) Predicted-True $z$ comparison}
  \end{minipage}%\\%
  \begin{minipage}{0.20\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/conf_matrix_rAGN_Stripe82.pdf}\hfill\break
    {(c) Radio-AGN confusion matrix}
  \end{minipage}%\\%
  \begin{minipage}{0.30\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/compare_redshift_rAGN_Stripe82.pdf}\hfill\break
    {(d) Predicted-True $z$ comparison}
  \end{minipage}%\\%
  \caption{Combined confusion matrices and True/predicted redshift density plot for the full radio AGN detection prediction computed using the testing sub-set from HETDEX (panels (a) and (b)) and the known labelled sources from Stripe\,82 (panels (c) and (d)).
  }
  \label{fig:conf_matx_results_radio_AGN}
\end{figure*}

\begin{table}
\setlength{\tabcolsep}{3pt}
\caption{Results of application of radio AGN prediction pipeline to the labelled sources in the HETDEX and Stripe\,82 fields.} % title of Table
\label{table:fit_radio_AGN_results}      % reference of table in the text
\centering                          % used for centering table
\resizebox{0.90\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
Subset                                  & Threshold & F$_{\beta}$ & MCC      & Precision  & Recall    \\
\hline
\multirow{2}{*}{HETDEX-test}            & Naive     & 0.2068    & 0.2493    & 0.5234    & 0.1379    \\
                                        & PR        & 0.3799    & 0.3366    & 0.3220    & 0.4461    \\
\multirow{2}{*}{S82-labelled}           & Naive     & 0.2408    & 0.2143    & 0.2544    & 0.2307    \\
                                        & PR        & 0.1942    & 0.1723    & 0.1133    & 0.4736    \\
\hline                                   %inserts single line
\end{tabular}
}
\end{table}


\begin{table*}
\setlength{\tabcolsep}{3pt}
\caption{Predicted and original properties for the $5$ sources in testing subset with the highest redshift predicted Radio AGN. Sources are sorted by decreasing predicted redshift. A description of the columns is presented in Appendix~\ref{sec:app_prediction_results}.}\label{table:pred_radio_AGN_known_HETDEX}
\centering
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{lrrrrrrrrrrrcr}
\hline
\hline
ID &     RA\_ICRS &    DE\_ICRS &  band\_num &  class &  Score\_AGN &  Prob\_AGN &  LOFAR\_detect &  Score\_radio &  Prob\_radio &  Score\_rAGN &  Prob\_rAGN &      $z$ &  pred\_$z$ \\
   &     (deg) &    (deg) &    &    &    &    &    &    &    &    &    &        &    \\
\hline
9898717  &  203.016113 &  55.518097 &         9 &    1.0 &   0.500082 &  0.954114 &             0 &     0.390861 &    0.375122 &    0.195462 &   0.357909 &  4.738 &  4.3679 \\
168686   &  164.769135 &  45.806320 &         8 &    1.0 &   0.500048 &  0.858157 &             0 &     0.450279 &    0.418719 &    0.225161 &   0.359326 &  4.893 &  4.1733 \\
14437074 &  213.226517 &  54.236343 &         9 &    1.0 &   0.500090 &  0.965187 &             0 &     0.251632 &    0.263746 &    0.125839 &   0.254564 &  4.326 &  4.0475 \\
10408176 &  188.163651 &  52.880898 &         9 &    1.0 &   0.500012 &  0.622448 &             0 &     0.604838 &    0.526003 &    0.302426 &   0.327410 &  4.340 &  3.9553 \\
12612753 &  227.216370 &  51.941029 &         9 &    1.0 &   0.500055 &  0.887909 &             0 &     0.364423 &    0.355080 &    0.182231 &   0.315278 &  3.795 &  3.8797 \\
\hline
\end{tabular}
}
\end{table*}


\begin{table*}
\setlength{\tabcolsep}{3pt}
\caption{Predicted and original properties for the $5$ sources in Stripe 82 with the highest predicted redshift on the labelled sources predicted to be Radio AGN. Sources are sorted by decreasing predicted redshift. A description of the columns is presented in Appendix~\ref{sec:app_prediction_results}.}\label{table:pred_radio_AGN_known_S82}
\centering
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{lrrrrrrrrrrrrr}
\hline
\hline
ID &     RA\_ICRS &   DE\_ICRS &  band\_num &  class &  Score\_AGN &  Prob\_AGN &  radio\_detect &  Score\_radio &  Prob\_radio &  Score\_rAGN &  Prob\_rAGN &      Z &  pred\_Z \\
  &     (deg) &   (deg) &    &    &    &   &    &    &    &    &    &       &    \\
\hline
1406323 &   32.679794 & -0.305035 &         6 &    1.0 &   0.500050 &  0.866373 &             1 &     0.185842 &    0.204867 &    0.092930 &   0.177491 &  4.650 &  4.4986 \\
326139  &   33.580879 & -1.121398 &         8 &    1.0 &   0.500040 &  0.822622 &             0 &     0.208769 &    0.225946 &    0.104393 &   0.185868 &  4.600 &  4.3785 \\
633752  &   12.526446 & -0.888660 &         9 &    1.0 &   0.500035 &  0.793882 &             0 &     0.206182 &    0.223600 &    0.103098 &   0.177512 &  4.310 &  4.2946 \\
2834844 &  344.101440 &  0.789000 &         7 &    1.0 &   0.500062 &  0.909395 &             0 &     0.375735 &    0.363709 &    0.187891 &   0.330756 &  4.099 &  4.0635 \\
3191865 &   31.881712 &  1.063655 &         9 &    1.0 &   0.500087 &  0.962260 &             0 &     0.264210 &    0.274477 &    0.132128 &   0.264118 &  3.841 &  4.0509 \\
\hline
\end{tabular}
}
\end{table*}


\begin{table*}
\setlength{\tabcolsep}{3pt}
\caption{Predicted and original properties for the $5$ sources in the HETDEX field with the highest predicted redshift on the unlabelled sources predicted to be Radio AGN. A description of the columns is presented in Appendix~\ref{sec:app_prediction_results}.}
\label{table:pred_radio_AGN_unknown_HETDEX}
\centering
\resizebox{0.90\textwidth}{!}{
\begin{tabular}{lrrrrrrrrrrr}
\hline
\hline
ID &   RA\_ICRS &  DE\_ICRS &  band\_num &  Score\_AGN &  Prob\_AGN &  radio\_detect &  Score\_radio &  Prob\_radio &  Score\_rAGN &  Prob\_rAGN &  pred\_Z \\
  &     (deg) &   (deg) &    &    &    &    &    &   &   &    &   \\
\hline
9544254  &  201.309235 &  53.746429 &         6 &   0.500007 &  0.578804 &             0 &     0.351672 &    0.345250 &    0.175838 &   0.199832 &  4.7114 \\
12355845 &  220.838120 &  50.319016 &         5 &   0.500007 &  0.578804 &             0 &     0.937123 &    0.794128 &    0.468568 &   0.459644 &  4.6064 \\
13814216 &  219.839142 &  52.660328 &         7 &   0.500015 &  0.650248 &             0 &     0.213846 &    0.230529 &    0.106926 &   0.149901 &  4.5622 \\
6698239  &  184.694901 &  49.063766 &         5 &   0.499995 &  0.467527 &             0 &     0.799085 &    0.662753 &    0.399538 &   0.309855 &  4.5483 \\
2951011  &  175.882446 &  55.497799 &         5 &   0.500008 &  0.589419 &             0 &     0.823295 &    0.681768 &    0.411654 &   0.401847 &  4.5320 \\
\hline
\end{tabular}
}
\end{table*}

\begin{table*}
\setlength{\tabcolsep}{3pt}
\caption{Predicted and original properties for the $5$ sources in Stripe 82 with the highest predicted redshift on the unlabelled sources predicted to be Radio AGN. A description of the columns is presented in Appendix~\ref{sec:app_prediction_results}.}\label{table:pred_radio_AGN_unknown_S82}
\centering
\resizebox{0.90\textwidth}{!}{
\begin{tabular}{lrrrrrrrrrrr}
\hline
\hline
ID &     RA\_ICRS &   DE\_ICRS &  band\_num &  Score\_AGN &  Prob\_AGN &  radio\_detect &  Score\_radio &  Prob\_radio &  Score\_rAGN &  Prob\_rAGN &  pred\_Z \\
  &   (deg) &  (deg) &    &   &    &   &    &    &    &   &    \\
\hline
3244450 &  26.276423 &  1.104065 &         7 &   0.500002 &  0.531172 &             0 &     0.542061 &    0.483128 &    0.271031 &   0.256624 &  4.3938 \\
1062270 &  11.744675 & -0.562642 &         7 &   0.499982 &  0.356043 &             0 &     0.196326 &    0.214586 &    0.098159 &   0.076402 &  4.3563 \\
3261269 &  28.882526 &  1.117103 &         7 &   0.500011 &  0.608660 &             0 &     0.354936 &    0.347777 &    0.177472 &   0.211678 &  4.3153 \\
1466227 &  18.157259 & -0.258997 &         5 &   0.500013 &  0.630968 &             0 &     0.456207 &    0.422973 &    0.228110 &   0.266882 &  4.3146 \\
1134866 &  11.304936 & -0.507943 &         7 &   0.500011 &  0.616439 &             0 &     0.226178 &    0.241539 &    0.113091 &   0.148894 &  4.3140 \\
\hline
\end{tabular}
}
\end{table*}

%--------------------------------------------------------------------
\section{Discussion}\label{sec:discussion}

\subsection{Comparison with previous prediction or detection works}\label{sec:compare_previous_works}
In this subsection, we provide a few examples of related published works as well as plausible explanations for observed discrepancies when these are present. This comparison attempts to be representative of the literature on the subject but does not intends to be complete in any way.

\subsubsection{AGN detection prediction}\label{sec:previous_AGN_detection}

We separate the comparison with previously published results between traditional and ML methodologies in order to understand the significance of our results and ways for future improvement.

Traditional AGN selection methods are based on the comparison of the measured Spectral Energy Distribution (SED) photometry to a template library \citep{2011Ap&SS.331....1W}. A recent example of its application is presented by  \citet{2022MNRAS.509.4940T} where best fit classifications were calculated for more than $700\,000$ galaxies in the D10 field of the Deep Extragalactic VIsible Legacy Survey \citep[DEVILS;][]{2018MNRAS.480..768D} and the Galaxy and Mass Assembly survey \citep[GAMA;][]{2011MNRAS.413..971D, 2015MNRAS.452.2087L}.
The $91 \%$ recovery rate of AGN, selected through various means (X-ray measurements, narrow and broad emission lines, and mid-infrared colours), is very much in line with our findings in Stripe 82, where our rate (recall) reaches $89 \%$.

Traditional methods also encompass the colour-based selection of AGN. While less precise, they provide access to a much larger base of candidates with a very low computational cost. We implemented some of the most common colour criteria on the data from Stripe\,82.
Of particular interest is the predicting power of the mid-IR colour selection due to its potential to detect hidden or heavily obscured AGN activity. 
 Based on WISE \citep{2010AJ....140.1868W} data, \citet[][S12]{2012ApJ...753...30S} proposed a threshold at W1~-~W2 $\geq 0.8$ to separate AGN from non-AGN using data from AGN in the COSMOS field \citep{2007ApJS..172....1S}.
A more stringent criterion was developed by \citet[][M12]{2012MNRAS.426.3271M}, the AGN wedge, which can be defined by the sources located inside the region defined by the relations W1~-~W2 $< 0.315 \times($W2~-~W3$)+ 0.791$, W1~-~W2 $> 0.315 \times($W2~-~W3$)- 0.222$, and W1~-~W2 $> -3.172 \times($ W2~-~W3 $)+ 7.624$. In order to define this wedge, they used data from X-ray selected AGN over an area of $44.43\, \mathrm{deg}^{2}$ in the northern sky.
\citet[][M16]{2016MNRAS.462.2631M} cross-correlated data from WISE observations with X-ray and radio surveys creating a sample of star-forming galaxies and AGN in the northern sky. They developed individual relations to separate classes of galaxies and AGN in the W1~-~W2, W2~-~W3 space and, for AGN the criterion, the relation is  W1~-~W2 $\geq 0.5$ and W2~-~W3 $< 4.4$.
More recently, \citet[][B18]{2018MNRAS.478.3056B} analysed the quality of mid-IR colour selection methods for the identification of obscured AGN involved in mergers. Using hydrodynamic simulations for the evolution of AGN in galaxy mergers, they developed a selection criterion from WISE colours which is shown to be able to separate, with high reliability, starburst galaxies from AGN. The expressions have the form W1~-~W2 $> 0.5$, W2~-~W3 $> 2.2$, and W1~-~W2 $> 2 \times($ W2~-~W3$) -8.9$.

The results from the application of these criteria to our samples in the testing subset and in the labelled sources of Stripe 82 field are summarised in Table~\ref{table:previous_AGN_methods} and a graphical representation of the boundaries they create in their respective parameter spaces is presented in Fig.~\ref{fig:W1_W2_W2_W3_AGN_pred_HETDEX_S82}. These figures are constructed as confusion matrices, plotting in each quadrant the whole WISE population in the background and in colour the corresponding fraction of the testing set (TP, TN, FP, and FN). As expected, our pipeline is able to separate with high confidence sources which are closer to the AGN or the galaxy locus (TP and TN) while the fraction of FN and FP lies somewhere in the middle. Additionally, it is possible to consider that the sources located in the FP quadrant of Fig.~\ref{fig:W1_W2_W2_W3_AGN_pred_HETDEX_S82} are likely AGN. They might show some degree of contamination on the original classification of sources. This work does not attempt to test the validity of the classification sources used for the training of our model. But, in the case of such contamination, it does not affect the overall results of our classifier as reflected in the precision values shown in Table~\ref{table:fit_AGN_results}. Only a $5\%$ of the sources in our samples are part of the FP region.

Overall, the poorer performance of the colour selection with respect to the model presented here is expected as the latter works in a larger parameter space. The lower performance of all the colour criteria presented in this section is compensated by the much reduced computational cost.

\begin{table}
\setlength{\tabcolsep}{3pt}
\caption{Results of application of several AGN detection criteria to our testing subset and the labelled sources from the Stripe 82 field.}             % title of Table
\label{table:previous_AGN_methods}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.62\columnwidth}{!}{
\begin{tabular}{c c c c c}        % centered columns (5 columns)
\hline\hline  
\multicolumn{5}{c}{HETDEX test set} \\
Method\tablefootmark{a} & F$_{\beta}$  & MCC    & Precision & Recall \\
\hline
S12       & 0.8610      & 0.7878    & 0.9398    & 0.8051 \\
M12       & 0.5180      & 0.4971    & 0.9887    & 0.3718 \\
M16       & 0.6721      & 0.6130    & 0.9748    & 0.5348 \\
B18       & 0.8214      & 0.7576    & 0.9754    & 0.7266 \\
This work & 0.9243      & 0.8717    & 0.9379    & 0.9133 \\
\hline\\[0.5em]
\hline\hline
\multicolumn{5}{c}{Stripe 82 (labelled)} \\
Method & F$_{\beta}$   & MCC       & Precision & Recall \\
\hline
S12       & 0.8359     & 0.4547    & 0.9393    & 0.7662 \\
M12       & 0.4680     & 0.2822    & 0.9959    & 0.3254 \\
M16       & 0.6469     & 0.3776    & 0.9880    & 0.5032 \\
B18       & 0.7971     & 0.5107    & 0.9872    & 0.6877 \\
This work & 0.9038     & 0.5742    & 0.9392    & 0.8764 \\
\hline
\end{tabular}
}
\tablefoot{
\tablefootmark{a}{Naming codes for the used methods are described in the main text (cf. Sect.~\ref{sec:previous_AGN_detection}). Last row of each sub-table corresponds to the criterion derived in this work (as described in Sect~\ref{sec:feat_importances}).}
}
\end{table}


\begin{figure*}
   \centering
   \script{fig_W1_W2_W3_conf_matrix_AGN.py}
   \includegraphics[width=0.65\textwidth]{figures/WISE_colour_colour_conf_matrix_AGN_HETDEX_test_S82_all.pdf}
   \caption{W1~-~W2, W2~-~W3 colour-colour diagrams for sources in the testing subset, from HETDEX, and in the labelled sources from Stripe 82 given their position in the AGN-galaxy confusion matrix (see, for HETDEX, rightmost panel of Fig.~\ref{fig:conf_matx_results_radio_AGN}). In the background, a density plot of all CW-detected sources in the full HETDEX field sample is displayed. The colour of each square represents the number of sources in that position of the parameter space, with darker squares having more sources (as defined in the colorbar of the upper-right panel). Contours represent the distribution of sources for each of the aforementioned subsets at $1$, $2$, and $3\,\sigma$ levels (shades of blue, for testing set and shades of red for labelled Stripe 82 sources). Coloured, solid lines display the limits from the criteria for the detection of AGN described in Sect.~\ref{sec:previous_AGN_detection}.}
   \label{fig:W1_W2_W2_W3_AGN_pred_HETDEX_S82}
\end{figure*}


For the case of ML-based models for AGN-galaxy classification, several analyses have been published in recent years. An example of their application is provided in \citet{2020A&A...639A..84C} where a Random Forest model for the classification of stars, galaxies and AGN using photometric data was trained from more than $3\,000\,000$ sources in the SDSS \citep[DR15;][]{2019ApJS..240...23A} and WISE with associated spectroscopic observations. Close to $400\,000$ sources have a quasar spectroscopic label and from the application of their model to a validation subset, they obtain a recall of $0.929$ and F1-score of $0.943$ for the quasar classification. These scores are of the same order as the ones obtained when applying our AGN-Galaxy model to the testing set (see Table~\ref{table:fit_AGN_results}). Thus, and despite using an order of magnitude fewer sources for the full training and validation process, our model can achieve equivalently good scores. 

Expanding on \citet{2020A&A...639A..84C}, \citet{2022A&A...666A..87C} built a ML pipeline, \texttt{SHEEP}, for the classification of sources into stars, galaxies and QSO. In contrast to \citet{2020A&A...639A..84C} or the pipeline described here, the first step in their analysis is the redshift prediction, which is used as part of the training features by the subsequent classifiers. They extracted WISE and SDSS \citep[DR15;][]{2019ApJS..240...23A} photometric data for almost $3\,500\,000$ sources classified as stars, galaxies or QSO. The application of their pipeline to sources predicted to be QSO led to a recall of $0.960$ and an F1 score of $0.967$. The improved scores in their pipeline might be a consequence not only of the slightly larger pool of sources, but also the inclusion of the coordinates of the sources (RA, Dec) and the predicted redshift values as features in the training. %In our case, we expect the coordinates to have a minimal impact because of the limited range in RA and Dec (${161.25\,\mathrm{deg} < \mathrm{RA} <232.5\,\mathrm{deg}}$; ${45\,\mathrm{deg} < \mathrm{Dec} < 57\,\mathrm{deg}}$). %\textit{These are indeed better scores than those shown by our pipeline when applied to sources not involved in training steps.}

A test with a larger number of ML methods was performed by \citet{2021A&A...651A.108P}. For training, they used optical and infrared data from close to $1\,500$ sources (galaxies and AGN) located at the AKARI North Ecliptic Pole (NEP) Wide-field \citep{2009PASJ...61..375L, 2012A&A...548A..29K} covering a $5.4\, \mathrm{deg}^{2}$ area. They tested  the following classifiers: \verb|LR|, \verb|SVM|, \verb|RF|, \verb|ET|, and \verb|XGBoost| including the possibility of generalised stacking. In general, they obtained results with F1-scores between $0.60$~--~$0.70$ and recall values in the range of $50\%$~--~$80\%$. 
These values, lower than the works described here, can be fully understood given the small size of the training sample. A larger photometric sample covers a wider range of the parameter space which significantly helps the metrics of any given model.

\subsubsection{Radio detection prediction}\label{sec:previous_radio_detection}

We have not found in the literature any work attempting the prediction of AGN radio detection at any level and therefore this is the first attempt at doing so. In the literature we do find several correlations between the AGN radio emission (flux) and that at other wavelengths \citep[e.g. with infrared emission,][]{1985ApJ...298L...7H, 1992ARA&A..30..575C} and substantial effort has been done towards classifying radio galaxies based upon their morphology \citep[e.g.][FRI, FRII, bent jets, etc.]{2017ApJS..230...20A, 2019MNRAS.482.1211W} and its connection to environment \citep{2008A&ARv..15...67M, 2022A&ARv..30....6M}. None of these extensive works has directly focused on the a priori presence or absence of radio emission above a certain threshold. Therefore, the results presented here are the first attempt at such an effort.

The ${\sim} 2$x success rate of the pipeline to identify radio emission in AGN (${\sim} 44.61\%$ recall and ${\sim} 32.20\%$ precision; see Table~\ref{table:fit_radio_AGN_results}) with the respect to a 'no-skill' or random (${\lessapprox}30 \%$) selection, provides the opportunity to understand what the model has learned from the data and, therefore, gain some insight into the nature or triggering mechanisms of the radio emission. We, therefore, reserve the discussion of the most important features, and the linked physical processes, driving the pipeline improved predictions to Sect.~\ref{sec:feat_importances}.


\subsubsection{Redshift value prediction}\label{sec:previous_z_values}

We compare our results to that of \citet[][Stripe 82X]{2017ApJ...850...66A} where the authors analysed multi-wavelength data from more than $6\,100$ X-ray detected AGN from the $31.3\, \mathrm{deg}^{2}$ of the Stripe 82X survey. They obtained photometric redshifts for almost $6\,000$ of these sources using the template-based fitting code \verb|LePhare| \citep{1999MNRAS.310..540A, 2006A&A...457..841I}. Their results present a normalised median absolute deviation of $\sigma_{\mathrm{NMAD}} = 0.062$ and an outlier fraction of $\eta = 13.69 \%$, values which are similar to our results in HETDEX and Stripe 82 except for a better outlier fraction (we obtain $\eta_{S82}=25.18\%$). A fairer comparison would require to look at the testing metrics in Table~\ref{table:fit_redshift_results} where both the $\sigma_{\mathrm{NMAD}}$ and outlier fraction show comparable results ($\sigma_{\mathrm{NMAD}} = 0.071$, $\eta = 18.6$\%).

On the ML side, we compare our results to those produced by \citet{2021Galax...9...86C}, with $\sigma_{\mathrm{NMAD}} = 0.1197$ and $\eta = 29.72 \%$, and find that our redshift prediction model improves by at least $25 \%$ for any given metric.
The source of improvement is probably related to the different sets of features used (colours vs ratios) and the more specific population of radio-AGN used to train our models.

Another example of the use of ML for AGN redshift prediction has been presented by \citet{2019PASP..131j8003L}. They studied the use of the k-nearest neighbours algorithm \verb|KNN| \citep{1053964}, a non-parametric supervised learning approach, to derive redshift values for radio-detectable sources. They combined $1.4$ GHz radio measurements, infrared, and optical photometry in the European Large Area ISO Survey-South 1 \citep[ELAIS-S1;][]{2000MNRAS.316..749O} and extended Chandra Deep Field South \citep[eCDFS;][]{2005ApJS..161...21L} fields, matching their sensitivities and depths to the expected values in the Evolutionary Map of the Universe \citep[EMU;][]{2011PASA...28..215N}. From the different experiments they run, their resulting NMAD values are in the range ${\sigma_{\mathrm{NMAD}} = 0.05 - 0.06}$, and their outlier fraction can be found between ${\eta = 7.35 \%}$ and ${\eta = 13.88 \%}$. 
As an extension to the previous results, \citet{LUKEN2022100557} analysed multi-wavelength data from radio-detected sources the eCDFS and the ELAIS-S1 fields. Using \texttt{KNN} and \texttt{RF} methods to predict the redshifts of more than $1\,300$ RGs, they have developed regression methods that show NMAD values between ${\sigma_{\mathrm{NMAD}} = 0.03}$ and ${\sigma_{\mathrm{NMAD}} = 0.06}$, ${\sigma_{z} = 0.10 - 0.19}$, and outlier fractions of ${\eta = 6.36 \%}$ and ${\eta = 12.75 \%}$.

In addition to the previous work, \citet{2019PASP..131j8004N} compared a number of methodologies, mostly related with ML but also the SED fitting algorithm \texttt{LePhare}, for predicting redshift values for radio sources. They have used more than $45$ photometric measurements (including $1.4$ GHz fluxes) from different surveys in the COSMOS field. From several settings of features, sensitivities, and parameters, they retrieved redshift predictions with NMAD values between ${\sigma_{\mathrm{NMAD}} = 0.054}$ and ${\sigma_{\mathrm{NMAD}} = 0.48}$ and outlier fractions that range between ${\eta = 7 \%}$ and ${\eta = 80 \%}$. The broad span of obtained values might be due to the combinations of properties for each individual training set (including the use of radio or X-ray measurements, the selection depth, and others) and to the size of these sets, which was small for ML purposes (less than $400$ sources). The slightly better results can be easily understood given the heavily populated photometric data available in COSMOS.

Specifically related to HETDEX, it is possible to compare our results to those from \citet{2019A&A...622A...3D}. They use a hybrid photometric redshift approach combining  traditional template fitting redshift determination and ML-based methods. In particular, they implemented a Gaussian Process (GP) algorithm, which is able to model both the intrinsic noise and the uncertainties of the training features. Their redshift prediction analysis of 
AGN sources with a spectroscopic redshift detected in the LoTSS DR1 ($6,811$ sources) found a NMAD value of ${\sigma_{\mathrm{NMAD}} = 0.102}$ and an outlier fraction of ${\eta = 26.6 \%}$.
The differences between these results and those obtained from the application of our models (individually and as part of the prediction pipeline) might be due to the differences in the creation of the training sets. \citet{2019A&A...622A...3D} use information from all available sources in the HETDEX field for training the redshift GP whilst our redshift model has been only trained on radio-detected AGN, giving it the opportunity to focus its parameter exploration only on these sources.

Finally, \citet{2022A&A...666A..87C} also produced photometric redshift predictions for almost $3\,500\,000$ sources (stars, galaxies, and QSO) as part of their pipeline (see Sect.~\ref{sec:previous_AGN_detection}). They combined three algorithms for their predictions: \texttt{XGBoost}, \texttt{CatBoost}, and \texttt{LightGBM} \citep{NIPS2017_6449f44a}. This procedure leads to ${\sigma_{\mathrm{NMAD}} = 0.018}$ and ${\eta = 2 \%}$. As with previous examples, the differences with our results can be a consequence of the number of training samples. Also, in the case of \citet{2022A&A...666A..87C}, they applied an additional post-processing step to the redshift predictions attempting to predict and understand the appearance of catastrophic outliers.

\subsection{Influence of data imputation}\label{sec:band_num_trends}

One effect which might influence the training of the models and, consequently, the prediction for new sources is related to the imputation of missing values (cf. Sect.~\ref{sec:data_collection}). In Fig.~\ref{fig:probs_band_num_test}, we have plotted the distributions of predicted scores (for classification models) and predicted redshift values as a function of the number of measured bands (\texttt{band\_num}) for each step of the pipeline as applied to the test sub-set.

\begin{figure}
  \centering
    \script{fig_scores_vs_band_num.py}
    \includegraphics[width=0.99\columnwidth]{figures/predicted_probs_band_num_HETDEX_test.pdf}
  \caption{Evolution of predicted probabilities (top: probability to be AGN, middle: probability of AGN to be detected in radio) and redshift values for radio-detectable AGN (bottom panel) as a function of the number of observed bands in all sources in the test set. Background density plot (following the colour coding in the colorbars) show the location of the predicted values. Overlaid boxplots display the main statistics for each number of measured bands. Black rectangles encompass sources in the second and the third quartiles. Vertical lines show the place of sources from the first and fourth quartiles. Orange lines represent the median value of the sample and dashed, green lines indicate their mean values. On top of each boxplot, the two written values correspond to the most relevant metric for the prediction (top and middle panels, recall, and bottom panel, $\sigma_{\mathrm{NMAD}}$) followed by the number of sources considered to create each set of statistics.}
  \label{fig:probs_band_num_test}
\end{figure}

The top panel of Fig.~\ref{fig:probs_band_num_test} shows the influence of the degree of imputation in the classification between AGN and galaxies. For most of the bins, predicted scores are distributed close to the extremes, without any noticeable trend (other than that given by the number of sources in each \texttt{band\_num} value). The selected metric, recall, shows a peak for intermediate values of \texttt{band\_num}.

The case of radio detection classification is somewhat different. For more than $8$ detected bands (i.e. $\mathtt{band\_num} > 8$), the predicted scores (and recall values) tend to increase towards a radio-detection prediction. This evolution might be an effect of an overall increase of the flux in every band which will, naturally, imply a higher probability of a radio detection as well.

Finally, a stronger effect can be seen with the evolution of predicted redshift values for radio-detectable AGN. Despite the lower number of available sources, it is possible to recognise that sources with higher number of available measurements are predicted to have lower redshift values and better selected metrics ($\sigma_{\mathrm{NMAD}}$). Sources that are closer to us have higher probabilities to be detected in a large number of bands. Thus, it is expected that our model predicts lower redshift values for the most measured sources in the field.

In consequence, Fig.~\ref{fig:probs_band_num_test} allows us to understand the influence of imputation over the predictions. The most highly affected quantity is the redshift, where large fractions of measured magnitudes are needed to obtain scores that are in line with previous results (cf. Sect.~\ref{sec:previous_z_values}). This effect is present in the radio detection prediction, but it is also influenced by the number of sources in the mid-\texttt{band\_num} values. The AGN-galaxy classification does not show any noticeable influence of imputation in its results and, rather, shows evidence of the effect of the number of sources in each \texttt{band\_num} bin.

\subsection{Model explanations}\label{sec:model_explain}

Given the success of the models and pipeline in classifying AGN, their radio detectability and redshift with the provided set of observables, knowing the relative weights that they have in the decision-making process is of utmost relevance. In this way, physical insight might be gained about the triggers of AGN and radio activity and its connection to their host.
Therefore, we have estimated both local and global feature importances for the individual models and the combined pipeline. Global importances were retrieved using the so-called `decrease in impurity' approach \citep[see, for example,][]{Breiman2001}. Local importances have been determined via Shapley values. A more detailed description of what these importances are and how they are calculated is given in the following sections.

\subsubsection{Global feature importances}\label{sec:feat_importances}

Overall, mean or global feature importances can be retrieved from models that are based on Decision Trees \citep[e.g. Random Forests and Boosting models,][]{Breiman2001, breiman2003manual}. All algorithms selected in this work (\verb|RF|, \verb|CatBoost|, \verb|XGBoost|, \verb|ET|, \verb|GBR|, and \verb|GBC|) belong to these two classes. For each feature, the decrease in impurity (a term frequently used in the literature related to Machine Learning) of the dataset is calculated for all the nodes of the tree in which that feature is used. Features with the highest impurity decrease will be more important for the model \citep{NIPS2013_e3796ae8}\footnote{For some models that are not based on Decision Trees, feature importances can be obtained from the coefficients that the training process delivers for each feature. These coefficients are related to the level to which each quantity is scaled to obtain a final prediction (as in the coefficients from a polynomial regression).}.

Insight into the decision-making of the pipeline can only rely on the specific weight of the original set of features (see Sect.~\ref{sec:feat_selection}). Table ~\ref{table:feat_importances} presents the ranked combined importances from the observables selected in each of the three sequential models that compose the pipeline. They have been combined using the importances from the meta-learner (as shown in Table~\ref{table:base_feat_importances}) and that of base-learners. The derived importances will be dependent on the dataset used, including any imputation for the missing data, and the details of the models, i.e. algorithms used and stacking procedure. 
We first notice in Table~\ref{table:feat_importances} that the order of the features is different for all three models. This difference reinforces the need, as stated in Sect.~\ref{sec:ML_training}, of developing separate models for each of the prediction stages of this work that would evaluate the best feature weights for the related classification or regression task. 

\begin{table}
\setlength{\tabcolsep}{2.9pt}
\caption{Relative importances (rescaled to add to $100$) for observed features from the three models combined between meta and base models.}             % title of Table
\label{table:feat_importances}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.97\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{AGN-Galaxy (meta-model: \texttt{CatBoost})} \\
Feature             & Importance    & Feature               & Importance    & Feature         & Importance \\
\hline
\texttt{W1\_W2}     & 68.945        & \texttt{H\_K}         & 1.715         & \texttt{z\_W2}  & 1.026      \\
\texttt{W1\_W3}     &  4.753        & \texttt{y\_W1}        & 1.659         & \texttt{z\_y}   & 0.722      \\
\texttt{g\_r}       &  4.040        & \texttt{y\_W2}        & 1.513         & \texttt{W3\_W4} & 0.669      \\
\texttt{r\_J}       &  4.006        & \texttt{i\_y}         & 1.441         & \texttt{W4mag}  & 0.558      \\
\texttt{r\_i}       &  3.780        & \texttt{i\_z}         & 1.366         & \texttt{H\_W3}  & 0.408      \\
\texttt{band\_num}  &  1.842        & \texttt{y\_J}         & 1.187         & \texttt{J\_H}   & 0.371      \\[0.5em]
\hline\hline
\multicolumn{6}{c}{Radio detection (meta-model: \texttt{GBC})} \\
Feature             & Importance    & Feature               & Importance    & Feature              & Importance \\
\hline
\texttt{W2\_W3}     &  9.609        & \texttt{y\_W1}        & 7.150         & \texttt{W4mag}      & 4.759      \\
\texttt{y\_J}       &  8.102        & \texttt{g\_r}         & 7.123         & \texttt{K\_W4}      & 2.280      \\
\texttt{W1\_W2}     &  8.010        & \texttt{z\_W1}        & 7.076         & \texttt{J\_H}       & 1.283      \\
\texttt{g\_i}       &  7.446        & \texttt{r\_z}         & 6.981         & \texttt{H\_K}       & 1.030      \\
\texttt{K\_W3}      &  7.357        & \texttt{i\_z}         & 6.867         & \texttt{band\_num}  & 1.018      \\
\texttt{z\_y}       &  7.321        & \texttt{r\_i}         & 6.588         &                     &            \\[0.5em]
\hline\hline
\multicolumn{6}{c}{Redshift prediction (meta-model: \texttt{ET})} \\
Feature               & Importance    & Feature           & Importance    & Feature           & Importance \\
\hline
\texttt{y\_W1}        & 35.572        & \texttt{y\_J}     & 3.018         & \texttt{i\_z}     & 1.215    \\
\texttt{W1\_W2}       & 13.526        & \texttt{r\_z}     & 3.000         & \texttt{J\_H}     & 1.162    \\
\texttt{W2\_W3}       & 12.608        & \texttt{r\_i}     & 2.896         & \texttt{g\_W3}    & 1.000    \\
\texttt{band\_number} &  6.358        & \texttt{z\_y}     & 2.827         & \texttt{K\_W3}    & 0.925    \\
\texttt{H\_K}         &  4.984        & \texttt{W4mag}    & 2.784         & \texttt{K\_W4}    & 0.762    \\
\texttt{g\_r}         &  4.954        & \texttt{i\_y}     & 2.408         &                   &          \\
\hline
\end{tabular}
}
%\tablefoot{Relative feature importance values are specific to each model training and cannot be compared, numerically, to the values obtained in a different model. A meaningful comparison can be done by contrasting the order in which features are sorted.}
\end{table}

\begin{table}
  \setlength{\tabcolsep}{2.9pt}
  \caption{Relative feature importances (rescaled to add to $100$) for base algorithms in each prediction step.}             % title of Table
  \label{table:base_feat_importances}      % is used to refer this table in the text
  \centering                          % used for centering table
  \resizebox{0.68\columnwidth}{!}{
  \begin{tabular}{c c c c}        % centered columns (4 columns)
  \hline\hline                 % inserts double horizontal lines   
  \multicolumn{4}{c}{AGN-Galaxy model (\texttt{CatBoost})} \\
  Feature             & Importance    & Feature           & Importance  \\
  \hline
  \texttt{gbc}        & 49.709        & \texttt{xgboost}  & 14.046      \\
  \texttt{et}         & 19.403        & \texttt{rf}       &  8.981      \\
  \multicolumn{3}{r}{Remaining feature importances:} & 7.861 \\[0.2em]
  \hline\hline
  \multicolumn{4}{c}{Radio detection model (\texttt{GBC})} \\
  
  Feature             & Importance    & Feature           & Importance \\
  \hline
  \texttt{rf}         & 12.024        & \texttt{catboost} & 7.137      \\
  \texttt{et}         &  7.154        & \texttt{xgboost}  & 6.604      \\
  %\hline
  \multicolumn{3}{r}{Remaining importances:} & 67.081 \\[0.2em]
  \hline\hline
  \multicolumn{4}{c}{Redshift prediction model (\texttt{ET})} \\
  
  Feature           & Importance    & Feature           & Importance \\
  \hline
  \texttt{xgboost}  & 25.138        & \texttt{catboost} & 21.072     \\
  \texttt{gbr}      & 21.864        & \texttt{rf}       & 13.709     \\
  %\hline
  \multicolumn{3}{r}{Remaining importances:} & 18.217 \\
  \hline
  \end{tabular}
  }
  %\tablefoot{Relative feature importance values are specific to each model training and cannot be compared, numerically, to the values obtained in a different model. A meaningful comparison can be done by contrasting the order in which features are sorted.}
  \end{table}

For the AGN-galaxy classification model, it is very interesting to note that the most important feature for the predicted probability of a source to be an AGN is the WISE colour W1~-~W2 (as well as W1~-~W3). This colour is indeed one of the axes of the widely used WISE colour-colour selection, with the second axis being the W2~-~W3 colour (cf. Sect~\ref{sec:previous_AGN_detection}). The WISE W3 photometry is though significantly less sensitive than W1, W2 or PS1 (see Fig.~\ref{fig:surveys_depth_HETDEX}) and a significant number of sources will be represented as upper limits in such plot (see Table~\ref{table:composition_catalogue}). From the importances in Table~\ref{table:feat_importances} and the values presented in Fig.~\ref{fig:hists_bands_nonimp_HETDEX_S82} we infer that using optical colours could in principle create selection criteria with metrics equivalent to those shown in Table~\ref{table:previous_AGN_methods} but for a much larger number of sources ($100\,000$ sources for colour plots using W3 vs $4\,700\,000$ sources for colours based in r, i or z magnitudes). We tested this hypothesis and derived a selection criterion in the r~-~z vs W1~-~W2 colour-colour plot shown in Fig.~\ref{fig:HETDEX_rz_W1W2_AGN_gal_class} using the labelled sources in the test sub-set of the HETDEX field. The results of the application of this criterion to the testing data and to the labelled sources in Stripe 82 is presented in the last row of Table~\ref{table:previous_AGN_methods}. Their limits are defined by the following expressions:

\begin{eqnarray}
r - z &>& -0.45\,,\\
r - z &<& 1.8\,,\\
W1 - W2 &>& 0.35 \times (r - z) + 0.26\,,
\end{eqnarray}

\noindent where W1, W2, r, and z are Vega magnitudes. Our colour criteria provides better and more homogeneous scores across the different metrics with purity (precision) and completeness (recall) above $87\%$. Avoiding the use of the longer WISE wavelength (W3 and W4), the criteria can be applied to a much larger dataset.


\begin{figure}
    \centering
    \begin{minipage}{0.99\columnwidth}
    \script{fig_W1_W2_r_z_AGN_gal_HETDEX.py}
    \includegraphics[width=\textwidth]{figures/r_z_W1_W2_AGN_gal_HETDEX_nonimputed.pdf}
    \end{minipage}%\\%
    \caption{AGN classification colour-colour plot in the HETDEX field using CW (W1, W2) and PS1 (r, z) passbands. Grey-scale density plot include all CW detected and non-imputed sources. Red contours highlight the density distribution of the AGN in the Million QSO catalogue (MQC) and blue contours show the density distribution for the galaxies from SDSS DR16. Contours are located at 1, 2, and 3 $\sigma$ levels.}
   \label{fig:HETDEX_rz_W1W2_AGN_gal_class}
\end{figure}

One of the main potential uses of the pipeline is its capability to pinpoint radio-detectable AGN. The global features analysis for the radio detection model shows a high dependence on the near- and mid-IR magnitudes and colours, especially those coming from WISE. As a useful outcome similar to the AGN-Galaxy classification, we can use the most relevant features to build useful plots for the pre-selection of these sources and get insight into the origin of the radio emission. This is the case for the W4 histogram, shown in Fig.~\ref{fig:hist_W4_nonimputed_pred_radio_non_radio_AGN}, where the radio-emitting AGN have systematically brighter measured W4 magnitudes. This added mid-IR flux might be simply due to an increased star formation rates (SFR) in these sources. In fact the $24\mu m$ flux is often used, together with that of H$\alpha$ as a proxy for SFR \citep{2009ApJ...703.1672K}. The radio detection for these sources might have a strong component linked to the ongoing SF, especially for the sources with real or predicted redshift below ${z {\sim} 1.5}$. A detailed exploration of the implications that these dependencies might have in our understanding of the triggering of radio emission on AGN, whether related to star formation (SF) or jets, is left for a future publication (Carvajal et al. in preparation).

\begin{figure}
  \centering
    \script{fig_W4_hist_rAGN.py}
    \includegraphics[width=0.99\columnwidth]{figures/hist_W4_radio_non_radio_HETDEX_S82_nonimputed.pdf}
  \caption{W4 magnitudes density distribution of the predicted radio-AGN  (clean histograms) in HETDEX (ochre histograms) and Stripe 82 (blue histograms) and W4 magnitudes from predicted AGN that are predicted to not have radio detection (dashed histograms).}
  \label{fig:hist_W4_nonimputed_pred_radio_non_radio_AGN}
\end{figure}

Finally, the redshift prediction model shows again that the final estimate is mostly driven by the results of the base learners, accounting for ${\sim} 82\%$ of the predicting power. The overall combined importance of features shows also in this case a strong dependence on several near-IR colours of which y~-~W1 and W1~-~W2 are the most relevant ones. 
The model still relies, to a lesser extent, on a broad range of optical features needed to trace the broad range of redshift possibilities ($z \in [0,6]$).% Understandably,  the high-$z$ model emphasises more on the information provided by the redder bands.


\subsubsection{Local feature importances: Shapley values}\label{sec:shapley_values}

As opposed to the global (mean) assessment of feature importances derived from the decrease in impurity, local (i.e. source by source) information on the performance of such features can be obtained from Shapley values. This is a method from coalitional game theory that tells us how to fairly distribute the dividends (the prediction in our case) among the features \citep{Shapley_article}. The previous statement means that the relative influence of each property from the dataset can be derived for individual predictions in the decision made by the model \citep[which is not the same as obtaining causal correlations between features and the target;][]{2020arXiv200805052M}. 
The combination of Shapley values with several other model explanation methods  was used by \citet[][]{NIPS2017_7062} to create the SHapley Additive exPlanations (SHAP) values. In this work, SHAP values were calculated using the python package \verb|SHAP|\footnote{\url{https://github.com/slundberg/shap}} and, in particular, its module for Tree-based predictors \citep{lundberg2020local2global}.
To speed calculations up, the package \verb|FastTreeSHAP|\footnote{\url{https://github.com/linkedin/fasttreeshap}} \citep[\texttt{v0.1.2};][]{2021arXiv210909847Y} was also used, which allows for multi-thread runs. 

One way to display these SHAP values is through the so-called decision plots. They can show how individual predictions are driven by the inclusion of each feature. Besides determining the most relevant properties that help the model make a decision, it is possible to detect sources that follow different prediction paths which could be, eventually and upon further examination, labelled as outliers. An example of this decision plot, linked to the AGN-Galaxy classification, is shown in Fig.~\ref{fig:SHAP_decision_AGN_meta_HETDEX_high_z} for a subsample of the high-redshift (${z \geq 4.0}$) spectroscopically classified AGN in the HETDEX field ($121$ sources, regardless of them being part of any sub-set involved in the training or validation of the models). The different features used by the meta-learner are stacked on the vertical axis with increasing weight and these final weight are sumarized in Table~\ref{table:base_shap_values}. Similarly, SHAP decision plots for the radio-detection and redshift prediction are presented in Figs.~\ref{fig:SHAP_decision_radio_meta_HETDEX_high_z} and \ref{fig:SHAP_decision_z_meta_HETDEX_high_z}, respectively.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.75\columnwidth}
    \script{SHAP/fig_decision_AGN_gal_hiz_AGN_HETDEX.py}
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_AGN_meta_learner_HETDEX_highz.pdf}
    \end{minipage}%\\%
    \caption{Decision plot from SHAP values for AGN-Galaxy classification from the $121$ high redshift ($z \geq 4$) spectroscopically confirmed AGN in HETDEX. Horizontal axis represents the model's output with a starting value for each source centred on the selected naive threshold for classification. Vertical axis shows features used in the model sorted, from top to bottom, by decreasing mean absolute SHAP value. Each prediction is represented by a coloured line corresponding to its final predicted value as shown by the colorbar at the top. Moving from the bottom of the plot to the top, SHAP values for each feature are added to the previous value in order to highlight how each feature contributes to the overall prediction. Predictions for sources detected by LOFAR are highlighted with a dotted, dashed line.}
   \label{fig:SHAP_decision_AGN_meta_HETDEX_high_z}
\end{figure}

\begin{table}[b]
\setlength{\tabcolsep}{2.9pt}
\caption{SHAP values (rescaled to add to $100$) for base algorithms in each prediction step for observed features using $121$ spectroscopically confirmed AGN at high redshift values ($z > 4$).}             % title of Table
\label{table:base_shap_values}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.68\columnwidth}{!}{
\begin{tabular}{c c c c}        % centered columns (4 columns)
\hline\hline                 % inserts double horizontal lines   
\multicolumn{4}{c}{AGN-Galaxy model (\texttt{CatBoost})}             \\
Feature             & SHAP value    & Feature           & SHAP value \\
\hline
\texttt{gbc}        & 36.250        & \texttt{rf}       & 21.835     \\
\texttt{et}         & 30.034        & \texttt{xgboost}  &  7.198      \\
\multicolumn{3}{r}{Remaining SHAP values:} & 4.683 \\[0.2em]
\hline\hline
\multicolumn{4}{c}{Radio detection model (\texttt{GBC})}        \\

Feature             & SHAP value    & Feature           & SHAP value \\
\hline
\texttt{rf}         & 11.423        & \texttt{catboost} &  5.696     \\
\texttt{xgboost}    &  7.741        & \texttt{et}       &  5.115     \\
%\hline
\multicolumn{3}{r}{Remaining SHAP values:} & 70.025 \\[0.2em]
\hline\hline
\multicolumn{4}{c}{Redshift prediction model (\texttt{ET})}          \\

Feature             & SHAP value    & Feature           & SHAP value \\
\hline
\texttt{xgboost}    & 41.191        & \texttt{gbr}      & 13.106     \\
\texttt{catboost}   & 20.297        & \texttt{rf}       & 11.648     \\
%\hline
\multicolumn{3}{r}{Remaining SHAP values:} & 13.758                  \\
\hline
\end{tabular}
}
%\tablefoot{SHAP values are specific to each model training and cannot be compared, numerically, to the values obtained in a different model and with different data. A meaningful comparison can be done by contrasting the order in which features are sorted.}
\end{table}

\begin{figure}
   \centering
   \begin{minipage}{0.75\columnwidth}
   \script{SHAP/fig_decision_radio_hiz_AGN_HETDEX.py}
   \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_radio_meta_learner_HETDEX_highz.pdf}
   \end{minipage}
   \caption{Decision plot from the SHAP values for all features from the radio detection model in the $121$ high redshift ($z \geq 4$) spectroscopically confirmed AGN from HETDEX. Description as in Fig.~\ref{fig:SHAP_decision_AGN_meta_HETDEX_high_z}.}
   \label{fig:SHAP_decision_radio_meta_HETDEX_high_z}
\end{figure}

\begin{figure}
   \centering
   \begin{minipage}{0.85\columnwidth}
   \script{SHAP/fig_decision_z_hiz_AGN_HETDEX.py}
   \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_z_meta_learner_HETDEX_highz.pdf}
   \end{minipage}
   \caption{Decision plot from the SHAP values for all features from the redshift prediction model in the $121$ high redshift ($z \geq 4$) spectroscopically confirmed AGN from HETDEX. Description as in Fig.~\ref{fig:SHAP_decision_AGN_meta_HETDEX_high_z}.}
   \label{fig:SHAP_decision_z_meta_HETDEX_high_z}
\end{figure}

As it can be seen, for the three models, base learners are amongst the features with the highest influence. This result raises the question of what drives these individual base predictions. Appendix~\ref{sec:app_shap_base} includes SHAP decision plots for all base learners used in this work. Additionally, and to be able to compare these results with the features importances from Sect.~\ref{sec:feat_importances}, we constructed Table~\ref{table:shap_values_combined}, which displays the combined SHAP values of base and meta learners but, in this case, for the same $121$ high-redshift confirmed AGN (with $29$ of them detected by LoTSS). Table~\ref{table:shap_values_combined} shows, as Table~\ref{table:feat_importances}, that the colour W1~-~W2 is the most important discriminator between AGN and Galaxies for this specific set of sources. The importance of the rest of the features is mixed: similar colours are located on the top spots (e.g. g~-~r, W1~-~W3 or r~-~i).

\begin{table}
\setlength{\tabcolsep}{2.9pt}
\caption{Combined and normalised (rescaled to add to $100$) mean absolute SHAP values for observed features from the three models using $121$ spectroscopically confirmed AGN at high redshift values ($z \geq 4$).}             % title of Table
\label{table:shap_values_combined}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.97\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{AGN-Galaxy model} \\
Feature             & SHAP value    & Feature             & SHAP value    & Feature         & SHAP value \\
\hline
\texttt{W1\_W2}     & 32.458        & \texttt{i\_y}       & 5.086         & \texttt{z\_y}   & 1.591      \\
\texttt{g\_r}       & 11.583        & \texttt{y\_W1}      & 4.639         & \texttt{H\_W3}  & 1.048      \\
\texttt{W1\_W3}     &  8.816        & \texttt{band\_num}  & 4.050         & \texttt{W4mag}  & 0.514      \\
\texttt{r\_i}       &  7.457        & \texttt{y\_W2}      & 3.228         & \texttt{H\_K}   & 0.466      \\
\texttt{i\_z}       &  6.741        & \texttt{z\_W2}      & 2.348         & \texttt{W3\_W4} & 0.466      \\
\texttt{r\_J}       &  6.613        & \texttt{y\_J}       & 1.718         & \texttt{J\_H}   & 0.178      \\[0.5em]
\hline\hline
\multicolumn{6}{c}{Radio detection model} \\
Feature             & SHAP value    & Feature             & SHAP value    & Feature             & SHAP value \\
\hline
\texttt{g\_i}     & 14.120        & \texttt{z\_W1}        & 6.751         & \texttt{W4mag}      & 2.691      \\
\texttt{W2\_W3}   & 13.201        & \texttt{r\_i}         & 5.577         & \texttt{band\_num}  & 2.661      \\
\texttt{g\_r}     & 12.955        & \texttt{r\_z}         & 5.161         & \texttt{K\_W4}      & 0.939      \\
\texttt{y\_J}     &  8.224        & \texttt{i\_z}         & 4.512         & \texttt{H\_K}       & 0.719      \\
\texttt{K\_W3}    &  7.441        & \texttt{z\_y}         & 4.121         & \texttt{J\_H}       & 0.190      \\
\texttt{W1\_W2}   &  6.874        & \texttt{y\_W1}        & 3.864         &                     &            \\[0.5em]
\hline\hline
\multicolumn{6}{c}{Redshift prediction model} \\
Feature             & SHAP value    & Feature             & SHAP value    & Feature         & SHAP value \\
\hline
\texttt{g\_r}       & 32.594        & \texttt{z\_y}       & 3.557         & \texttt{W4mag}  & 1.639    \\
\texttt{y\_W1}      & 20.770        & \texttt{y\_J}       & 3.010         & \texttt{g\_W3}  & 1.479    \\
\texttt{W2\_W3}     & 12.462        & \texttt{band\_num}  & 2.595         & \texttt{K\_W3}  & 0.853    \\
\texttt{W1\_W2}     &  5.692        & \texttt{i\_y}       & 2.381         & \texttt{K\_W4}  & 0.451    \\
\texttt{r\_i}       &  4.381        & \texttt{H\_K}       & 2.230         & \texttt{J\_H}   & 0.146    \\
\texttt{r\_z}       &  3.755        & \texttt{i\_z}       & 2.005         &                 &          \\
\hline
\end{tabular}
}
\end{table}

For the radio classification step of the pipeline, we find that features linked to those $121$ high-$z$ AGN perform at the same level as for the overall population. %As introduced in Sect.~\ref{sec:results_radio}, radio-detection model shows difficulties when producing a classification based on the provided dataset. This issue is reflected in the narrow decision margin for the non-calibrated stacked model (see model output values --x-axis-- close to $\sim0.5$ in Fig.~\ref{fig:SHAP_decision_radio_meta_HETDEX_high_z}). 
The improved metrics with respect to those obtained from the 'no-skill' selection do indicate that the model has learned some connections between the data and the radio emission. Feature importance has changed when compared to the overall population. If the radio emission observed from these sources were exclusively due to SF, this connection would imply SFR of several hundred $M_\sun$\,yr$^{-1}$. This explanation can not be completely ruled out from the model side but some contribution of radio emission from the AGN is expected. The detailed analysis of the exact contribution for the SF and AGN component will be left for a forthcoming publication (Carvajal et al. in preparation). 

%Another relevant observation is that, unlike the radio meta learner, radio base models show that their predicted scores range between $0$ and $1$. These results reinforce the argument that using ensemble ML can help improving predictions and insights that can be obtained from the models.

%\subsection{Expectactions for future surveys}\label{sec:future_surveys}

%Additionally, the pipeline can be tuned to different radio surveys. This change needs to use the corresponding data in the training set. Thus, for example, radio surveys with different depths can be combined to produce the most sensitive but also most extensive radio-AGN prediction.

%Finally, and by providing radio detectability scores, the pipeline can be used, together with other methodologies, to pinpoint the correct multi-wavelength counterparts, especially for surveys with larger beams.

 
%--------------------------------------------------------------------
\section{Summary and conclusions}\label{sec:summary_conclusions}

%In this work, we have created a series of Machine Learning models in order to produce a set of predicted detections of radio AGN, along with their predicted photometric redshift values.

In this paper, we have shown that it is possible to build a pipeline to detect AGN, determine their detectability in radio, within a given flux limit, and their redshift value. Most importantly, we have described a series of methodologies to understand the driving properties of the different decisions, in particular for the radio detection which is, to our best knowledge, the first attempt at doing so.

For training the models, we used multi-wavelength data from almost $120\,000$ spectroscopically identified infrared-detected sources in the HETDEX field (which contains, in total, more than $15\,000\,000$ detections) and created stacked models, which used the input from base algorithms to improve the knowledge available to a meta-learner and arrive to more reliable predictions.

These models were applied, sequentially, to $15\,018\,144$ infrared detections in the HETDEX Spring field that do not show a spectroscopic classification as either galaxy or AGN, arriving to the creation of $68\,252$ radio AGN candidates with their corresponding predicted redshift values. Additionally, we applied the models to $3\,568\,478$ infrared detections in the Stripe 82 field with no spectroscopic classification, obtaining $22\,445$ new radio AGN candidates with their predicted redshift values.

We have, then, applied a number of analyses on the models to understand the influence of the observed properties from the studied sources over the predictions and their confidence levels. In particular, the use of SHAP values gives the opportunity to extract the influence that the feature set has for each individual prediction.

From the application of the prediction pipeline on labelled and unlabelled sources and the analysis of the predictions and the models themselves, the following conclusions can be drawn.

\begin{itemize}
%\item In general, the use of Machine Learning techniques allows to obtain results in a fraction of the time that traditional methods could take and allow for the extraction of information from very large datasets. In some cases, this includes the training times.
\item Generalised stacking is a useful procedure which collects results from individual ML algorithms into a single model that can outperform each of the individual models. This technique also helps prevent the inclusion of biases introduced by the use of individual algorithms. We have shown that the inclusion of generalised stacking in the identification of radio-detectable AGN can help improve the detection rates in the classification steps and the prediction values in the regression stage of our pipeline.
\item Classification between AGN and galaxies derived from our model is in line with previous works. Our pipeline is able to retrieve $96\%$ of previously-classified AGN from HETDEX (recall $= 0.9621$, precision $= 0.9449$, F$_{\beta} = 0.9542$, MCC $= 0.9185$) and $94\%$ of AGN in the Stripe 82 field (recall $= 0.9401$, precision $= 0.9481$, F$_{\beta} = 0.9437$, MCC $= 0.7067$).
\item Radio detection classification for predicted AGN has proven to be highly demanding in terms of data needed for creating the models. Thanks to the use of the techniques shown in this article (i.e. feature creation and selection, generalised stacking, probability calibration, and threshold optimisation), we are able to retrieve $52\%$ of previously-known radio-detectable AGN in the HETDEX field (recall $= 0.5216$, precision $= 0.3528$, F$_{\beta} = 0.4288$, MCC $= 0.3247$) and $58\%$ of these sources in the Stripe 82 field (recall $= 0.5816$, precision $= 0.1229$, F$_{\beta} = 0.2162$, MCC $= 0.1937$). These rates improve upon a purely random selection.
\item The prediction of redshift values for sources classified to be radio-detectable AGN can deliver results that are in line with works that use either traditional or ML methods.
\item Our models (classification and regression) can be applied to areas of the sky which have different radio coverage from that used for training without a strong degradation of the prediction results. This feature can lead to the use of our pipeline over very distinct datasets (in radio and multi-wavelength coverage) expecting to recover the sources predicted to be radio-detectable AGN with a high probability.
\item Machine Learning models cannot be only used for a direct prediction of a value (or a set of value). They can also be subject to analyses that allow to extract additional results. We took advantage of this fact by using global and local feature importances to derive novel colour-colour AGN selection methods.
\end{itemize}

%We aim to use the pipeline presented in this article in areas of the sky which will be covered by future surveys in radio wavelengths (e.g. RACS, EMU, and MIGHTEE). Additionally, it is possible to improve and expand this pipeline to include and predict additional measurements.

With the next generation of observatories already producing source catalogues with an order of magnitude better sensitivity over large areas of the sky than previously \citep[e.g. RACS, EMU, and MIGHTEE; ][respectively]{2020PASA...37...48M, 2011PASA...28..215N, 2016mks..confE...6J}, the need to understand the fraction of those radio detections related to AGN and determine counterparts across wavelengths is more necessary than ever. Radio emission from AGN has important implications for the accretion history of the universe, the triggers and conditions of radio emission and the connection to the host.

Although we developed the pipeline as a tool to better understand the aforementioned issues, we foresee additional possibilities in which the pipeline can be of great use. The first of this possibilities involves the use of the pipeline to assist with the selection of radio-detectable AGN within any set of observations. This application might turn particularly valuable in recent surveys carried out with MeerKAT \citep{2016mks..confE...1J} or the future SKA where the population at the faintest sources will be dominated by star-forming galaxies. This change needs to use the corresponding data in the training set.

Future developments of the pipeline will concentrate on minimising the existent biases in the training sample as well as in increasing the coverage of the parameter space. We also plan to generalise the pipeline to make it useful for non-radio or galaxy-related research communities. These developments include, for instance, the capability to carry the full analysis for the galactic and stellar populations (i.e. models to determine if a galaxy can be detected in the radio and to predict redshift values for galaxies and non-radio AGN). Another development includes improved analyses of the redshift predictions with the inclusion of confidence levels or probability distributions.

In order to implement these changes, we are also working on the extension of the parameter space of our training sets by including information from radio surveys with different characteristics. Namely, shallower, but with larger area, and less extended but with deeper multi-wavelength data. Similarly, the inclusion of far-IR, X-ray, and multi-survey radio measurements makes part of our efforts to improve detections, not only in radio, but in additional wavelengths.

\begin{acknowledgements}
The authors thank insightful comments from P. Papaderos and B. Arsioli.
This work was supported by FundaÃ§Ã£o para a CiÃªncia e a Tecnologia (FCT) through the research grants PTDC/FIS-AST/29245/2017, EXPL/FIS-AST/1085/2021, UID/FIS/04434/2019, UIDB/04434/2020, and UIDP/04434/2020. R.C. acknowledges support from the FundaÃ§Ã£o para a CiÃªncia e a Tecnologia (FCT) through the Fellowship PD/BD/150455/2019 (PhD:SPACE Doctoral Network PD/00040/2012) and POCH/FSE (EC). A.H. acknowledges support from contract DL 57/2016/CP1364/CT0002 and an FCT-CAPES funded Transnational Cooperation project ``Strategic Partnership in Astrophysics Portugal-Brazil''.
This publication makes use of data products from the Wide-field Infrared Survey Explorer, which is a joint project of the University of California, Los Angeles, and the Jet Propulsion Laboratory/California Institute of Technology, funded by the National Aeronautics and Space Administration.
LOFAR data products were provided by the LOFAR Surveys Key Science project (LSKSP\footnote{\url{https://lofar-surveys.org/}}) and were derived from observations with the International LOFAR Telescope (ILT). LOFAR \citep{2013A&A...556A...2V} is the Low Frequency Array designed and constructed by ASTRON. It has observing, data processing, and data storage facilities in several countries, which are owned by various parties (each with their own funding sources), and which are collectively operated by the ILT foundation under a joint scientific policy. The efforts of the LSKSP have benefited from funding from the European Research Council, NOVA, NWO, CNRS-INSU, the SURF Co-operative, the UK Science and Technology Funding Council and the JÃ¼lich Supercomputing Centre.
The Pan-STARRS1 Surveys (PS1) and the PS1 public science archive have been made possible through contributions by the Institute for Astronomy, the University of Hawaii, the Pan-STARRS Project Office, the Max-Planck Society and its participating institutes, the Max Planck Institute for Astronomy, Heidelberg and the Max Planck Institute for Extraterrestrial Physics, Garching, The Johns Hopkins University, Durham University, the University of Edinburgh, the Queen's University Belfast, the Harvard-Smithsonian Center for Astrophysics, the Las Cumbres Observatory Global Telescope Network Incorporated, the National Central University of Taiwan, the Space Telescope Science Institute, the National Aeronautics and Space Administration under Grant No. NNX08AR22G issued through the Planetary Science Division of the NASA Science Mission Directorate, the National Science Foundation Grant No. AST-1238877, the University of Maryland, E\"{o}tv\"{o}s Lor\'{a}nd University (ELTE), the Los Alamos National Laboratory, and the Gordon and Betty Moore Foundation.
This publication makes use of data products from the Two Micron All Sky Survey, which is a joint project of the University of Massachusetts and the Infrared Processing and Analysis Center/California Institute of Technology, funded by the National Aeronautics and Space Administration and the National Science Foundation.
This work made use of public data from the Sloan Digital Sky Survey, Data Release 16. Funding for the Sloan Digital Sky Survey IV has been provided by the Alfred P. Sloan Foundation, the U.S. Department of Energy Office of Science, and the Participating Institutions. 
SDSS-IV acknowledges support and resources from the Center for High Performance Computing  at the University of Utah. The SDSS website is \url{www.sdss.org}.
SDSS-IV is managed by the Astrophysical Research Consortium for the Participating Institutions of the SDSS Collaboration including the Brazilian Participation Group, the Carnegie Institution for Science, Carnegie Mellon University, Center for Astrophysics | Harvard \& Smithsonian, the Chilean Participation Group, the French Participation Group, Instituto de Astrof\'isica de Canarias, The Johns Hopkins University, Kavli Institute for the Physics and Mathematics of the Universe (IPMU) / University of Tokyo, the Korean Participation Group, Lawrence Berkeley National Laboratory, Leibniz Institut f\"ur Astrophysik Potsdam (AIP),  Max-Planck-Institut f\"ur Astronomie (MPIA Heidelberg), Max-Planck-Institut f\"ur Astrophysik (MPA Garching), Max-Planck-Institut f\"ur Extraterrestrische Physik (MPE), National Astronomical Observatories of China, New Mexico State University, New York University, University of Notre Dame, Observat\'ario Nacional / MCTI, The Ohio State University, Pennsylvania State University, Shanghai Astronomical Observatory, United Kingdom Participation Group, Universidad Nacional Aut\'onoma de M\'exico, University of Arizona, University of Colorado Boulder, University of Oxford, University of Portsmouth, University of Utah, University of Virginia, University of Washington, University of Wisconsin, Vanderbilt University, and Yale University.
This research has made use of NASAâs Astrophysics Data System, TOPCAT\footnote{\url{http://www.star.bris.ac.uk/~mbt/topcat/}} \citep{2005ASPC..347...29T}, JupyterLab\footnote{\url{https://jupyter.org}} \citep{jupyter}, "Aladin sky atlas" \citep[\texttt{v11.0.24};][]{2000A&AS..143...33B} developed at CDS, Strasbourg Observatory, France, and the VizieR catalogue access tool, CDS, Strasbourg, France (DOI : 10.26093/cds/vizier). The original description of the VizieR service was published in \cite{vizier}.
This work made extensive use of the Python packages \texttt{PyCaret}\footnote{\url{https://pycaret.org}} \citep[\texttt{v2.3.10};][]{PyCaret}, \texttt{scikit-learn} \citep[\texttt{v0.23.2};][]{scikit-learn}, \texttt{pandas}\footnote{\url{https://pandas.pydata.org}} \citep[\texttt{v1.4.2};][]{pandas}, Astropy\footnote{\url{https://www.astropy.org}}, a community-developed core Python package for Astronomy \citep[\texttt{v5.0};][]{astropy:2013, astropy:2018, 2022ApJ...935..167A}, \texttt{Matplotlib} \citep[\texttt{v3.5.1};][]{Hunter:2007}, \texttt{betacal}\footnote{\url{https://betacal.github.io}} (\texttt{v1.1.0}), \texttt{ChainConsumer}\footnote{\url{https://github.com/Samreay/ChainConsumer}} \citep[\texttt{v0.34};][]{Hinton2016}, \texttt{mpl-scatter-density}\footnote{\url{https://github.com/astrofrog/mpl-scatter-density}} (\texttt{v0.7}), and \texttt{CMasher}\footnote{\url{https://github.com/1313e/CMasher}} \citep[\texttt{v1.6.3};][]{2020JOSS....5.2004V}.
\end{acknowledgements}

% WARNING
%-------------------------------------------------------------------
% Please note that we have included the references to the file aa.dem in
% order to compile it, but we ask you to:
%
% - use BibTeX with the regular commands:
%   \bibliographystyle{aa} % style aa.bst
%   \bibliography{Yourfile} % your references Yourfile.bib
%
% - join the .bib files when you upload your source files
%-------------------------------------------------------------------


\bibliographystyle{bibtex/aa} % style aa.bst
\bibliography{bibliog} % your references Yourfile.bib


\begin{appendix} %First appendix

\section{Column names in this study}\label{sec:app_feature_names}

Table~\ref{table:feature_names_in_work} presents the names (and what they represent) of the features, used in throughout this work. This information can be read in combination with the columns presented in Appendix~\ref{sec:app_prediction_results}.

\begin{table}
\setlength{\tabcolsep}{0.5pt}
\caption{Names of columns or features used in the code and what they represent.}             % title of Table
\label{table:feature_names_in_work}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.99\columnwidth}{!}{
\begin{tabular}{c c c c c c}        % centered columns (6 columns)
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{Photometry measurements (magnitudes and fluxes)} \\
Code name       & Feature   & Code name     & Feature   & Code name         & Feature \\
\hline
\texttt{gmag}   & g (PS1)   & \texttt{ymag} & y (PS1)   & \texttt{W1mproPM} & W1 (CW) \\
\texttt{rmag}   & r (PS1)   & \texttt{Jmag} & J (2M)    & \texttt{W1mproPM} & W2 (CW) \\
\texttt{imag}   & i (PS1)   & \texttt{Hmag} & H (2M)    & \texttt{W3mag}    & W3 (AW) \\
\texttt{zmag}   & z (PS1)   & \texttt{Kmag} & Ks (2M)   & \texttt{W4mag}    & W4 (AW) \\[0.25em]
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{Colours} \\
\hline
\multicolumn{6}{c}{$66$ colours from all combinations of non-radio magnitudes.}\\
\multicolumn{6}{c}{A sub-sample of them is shown.}\\
\texttt{g\_r}    & g - r (PS1) & \ldots & \ldots & \texttt{W2\_W3} & W2 (CW) - W3 (AW) \\
\texttt{g\_i}    & g - i (PS1) & \ldots & \ldots & \texttt{W2\_W4} & W2 (CW) - W4 (AW) \\
\texttt{g\_z}    & g - z (PS1) & \ldots & \ldots & \texttt{W3\_W4} & W3 - W4 (AW) \\[0.25em]
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{Categorical flags} \\
Code name                           & \multicolumn{2}{c}{Feature}           &   &   &   \\
\hline
\multirow{2}{*}{\texttt{band\_num}} & \multicolumn{2}{c}{Number of bands}   &   &   &   \\
                                    & \multicolumn{2}{c}{with measurements} &   &   &   \\[0.25em]
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{Boolean flags} \\
Code name       & Feature       & Code name                 & \multicolumn{3}{c}{Feature}                                   \\
\hline
\texttt{class}  & AGN or galaxy & \texttt{radio\_detect}    & \multicolumn{3}{c}{Detection in, at least, one radio band.}   \\[0.25em]
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{Redshift} \\
Code name   & \multicolumn{2}{c}{Feature}                   &   &   &   \\
\hline
\texttt{Z}  & \multicolumn{2}{c}{Spectroscopic redshift}    &   &   &   \\[0.25em]
\hline\hline                 % inserts double horizontal lines   
\multicolumn{6}{c}{Outputs of base models} \\
Code name           & Feature       & Code name                     & Feature           & Code name                     & Feature           \\
\hline
\texttt{XGBoost}    & XGBoost       & \texttt{ET}                   & Extra Trees       & \multirow{2}{*}{\texttt{GBR}} & Gradient Boosting \\
\texttt{CatBoost}   & CatBoost      & \multirow{2}{*}{\texttt{GBC}} & Gradient Boosting &                               & Regressor         \\
\texttt{RF}         & Random Forest &                               & Classifier        &                               &                   \\
\hline                                   %inserts single line
\end{tabular}
}
\end{table}


\section{Non-imputed magnitudes}\label{sec:app_nonimputed_mag_dist}

The number of valid measurements in Fig.~\ref{fig:hists_bands_nonimp_HETDEX_S82} for each field and band can be used to determine the relative difference of density of sources between both fields. This density can be obtained by dividing the number of valid measurements over the effective area of each field (Sect.~\ref{sec:data}). Table~\ref{table:magnitude_density} shows these densities.

\begin{table}
\setlength{\tabcolsep}{2.9pt}
\caption{Density of detected sources (in units of sources per square degree) per band and field.}             % title of Table
\label{table:magnitude_density}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.90\columnwidth}{!}{
\begin{tabular}{c c c c c c c c}        % centered columns (8 columns)
\hline\hline                 % inserts double horizontal lines   
\multicolumn{8}{c}{HETDEX Field} \\
Band    & Density         & Band  & Density         & Band  & Density         & Band  & Density       \\
        & (deg$^{-2}$)    &       & (deg$^{-2}$)    &       & (deg$^{-2}$)    &       & (deg$^{-2}$)  \\
\hline
g       &  6\,380.66      & z     & 10\,331.93      & H     &  1\,335.55      & W2    & 35\,700.18    \\
r       &  9\,304.58      & y     &  6\,735.97      & Ks    &  1\,335.55      & W3    & 14\,045.08    \\
i       & 11\,242.35      & J     &  1\,335.55      & W1    & 35\,700.18      & W4    & 14\,044.78    \\[0.25em]
\hline\hline
\multicolumn{8}{c}{Stripe 82 Field} \\
Band    & Density         & Band  & Density         & Band  & Density         & Band  & Density       \\
        & (deg$^{-2}$)    &       & (deg$^{-2}$)    &       & (deg$^{-2}$)    &       & (deg$^{-2}$)  \\
\hline
g       &  8\,249.04      & z     & 13\,214.70      & H     &  2\,330.92      & W2    & 39\,025.05    \\
r       & 12\,962.35      & y     &  9\,226.45      & Ks    &  2\,330.92      & W3    & 15\,393.12    \\
i       & 14\,507.01      & J     &  2\,330.92      & W1    & 39\,025.01      & W4    & 15\,472.75    \\
\hline
\end{tabular}
}
\end{table}


\section{From Scores to Probabilities}\label{app:calibration_models}

\begin{figure}
  \centering
  \begin{minipage}{0.24\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/calib_curves_pre_calib_AGN_galaxy.pdf}\hfill\break%\linebreak
    {(a) AGN-galaxy}
  \end{minipage}
  \hfill 
  \begin{minipage}{0.24\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/calib_curves_pre_calib_radio.pdf}\hfill\break%\linebreak
    {(b) Radio detection}
  \end{minipage}
  \caption{Reliability curves for uncalibrated classifiers. Each line represents the calibration curve for each subset in HETDEX field. Data has been binned and each bin (represented by the points) has the same number of elements per curve. Dashed line represents a perfectly calibrated model. (a) AGN-galaxy classification model. (b) Radio detection model.}
  \label{fig:calibration_curves_classification_pre}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{0.24\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/calib_curves_post_calib_AGN_galaxy.pdf}\hfill\break%\linebreak
    {(a) AGN-galaxy}
  \end{minipage}
  \hfill 
  \begin{minipage}{0.24\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/calib_curves_post_calib_radio.pdf}\hfill\break%\linebreak
    {(b) Radio detection}
  \end{minipage}
  \caption{Reliability curves for calibrated classifiers. (a) AGN-galaxy classification model. (b) Radio detection model. Details as in Fig.~\ref{fig:calibration_curves_classification_pre}}
  \label{fig:calibration_curves_classification_post}
\end{figure}

In general, classifiers deliver scores in the range [$0$, $1$], which could be associated to the probability of a studied source being part of the relevant class (in our work, AGN or radio detectable). The classifier uses a threshold above which, any predicted element would be considered a positive instance. 

With the exception of few algorithms (including the family of logistic regressions), scores from classifiers cannot be directly used as probabilities. As a consequence of this inability, such values cannot be compared from one type of model to some other and can not be combined to obtain a joint score.
Therefore, in order to retrieve joint scores and treat them as probabilities, scores (and, by extension, the classifiers) need to be calibrated. This calibration means that, when taking all predictions with a probability $P$ of being of a class, a fraction $P$ of them really belong to that class \citep[e.g.][]{lichtenstein_1982, 2021arXiv211210327S}.

Calibration of these scores can be done by applying a transformation to their values. For our work, we will apply a Beta transformation. It allows to re-distribute the scores of a classifier allowing them to get closer to the definition of probability \citep{10.1214/17-EJS1338SI, pmlr-v54-kull17a}. Calibration steps in our workflow have been applied using the Python package \verb|betacal|. In the case of the radio detection model, the new scores have a wider range than the original, uncalibrated scores.%, but still not covering the full range $[0,1]$, a sign of the lack of strong confidence in the stacked model to predict the radio detection of sources.

When obtaining the BSS values for both classification, the AGN-Galaxy classifier has a score of ${\mathrm{BSS} = -0.002}$, demonstrating that no major changes were applied to the distribution of scores. For the radio detection classifier, the score is ${\mathrm{BSS} = -0.434}$. Even though the BSS value is slightly negative for the AGN-Galaxy classifier, we keep it since its range of values now can be compared and combined with additional probabilities. In the case of the radio detection classifier, the BSS shows a degradation of the calibration, but we will keep the calibrated model given that it provides, overall, better values for the remaining metrics. This effect can be seen, more strongly, with recall.

Calibration (or reliability) plots show how well calibrated the predicted scores of a classifier are by displaying the fraction of sources that are part of a given class as a function of the predicted probability. A perfectly calibrated classifier would have all its prediction lying in the ${x{=}y}$ line. The magnitude of the deviations from that line give information of the miscalibration a model has \citep[see, for instance,][]{ReliabilityofReliabilityDiagrams, VanCalster2019}. In Fig.~\ref{fig:calibration_curves_classification_pre}, we present the reliability curves for the uncalibrated classifiers and, in Fig.~\ref{fig:calibration_curves_classification_post}, for their calibrated versions.

\section{Meta-learners hyper-parameters}\label{sec:app_hyperpars}

\begin{table}
\setlength{\tabcolsep}{2.9pt}
\caption{Hyper-parameters values for meta-learners after tuning.}             % title of Table
\label{table:hyper_params_meta}      % is used to refer this table in the text
\centering                          % used for centering table
\resizebox{0.93\columnwidth}{!}{
\begin{tabular}{c c c c}        % centered columns (4 columns)
\hline\hline                 % inserts double horizontal lines   
\multicolumn{4}{c}{AGN-Galaxy model (\texttt{CatBoost})}                                                \\
Parameter                           & Value             & Parameter                     & Value         \\
\hline
\texttt{learning\_rate}             & 0.0075            & \texttt{random\_strength}     & 0.1           \\
\texttt{depth}                      & 6                 & \texttt{l2\_leaf\_reg}        & 10            \\[0.2em]
\hline\hline
\multicolumn{4}{c}{Radio detection model (\texttt{GradientBoosting})}                                           \\

Parameter                           & Value             & Parameter                     & Value         \\
\hline
\texttt{n\_estimators}              & 187               & \texttt{min\_samples\_leaf}   & 2             \\
\texttt{learning\_rate}             & 0.0560            & \texttt{max\_depth}           & 9             \\
\texttt{subsample}                  & 0.3387            & \texttt{max\_features}        & 0.5248        \\
\texttt{min\_samples\_split}        & 5                 &                               &               \\[0.2em]
\hline\hline
\multicolumn{4}{c}{Redshift prediction model (\texttt{ET})}                                             \\

Parameter                           & Value             & Parameter                     & Value         \\
\hline
\texttt{n\_estimators}              & 100               & \texttt{criterion}            & \texttt{mae}  \\
\texttt{max\_depth}                 & \texttt{None}     & \texttt{min\_samples\_split}  & 2             \\
\texttt{max\_features}              & \texttt{auto}     & \texttt{min\_samples\_leaf}   & 1             \\
\texttt{bootstrap}                  & \texttt{False}    &                               &               \\
\hline
\end{tabular}
}
\tablefoot{This table shows the parameters which were subject to tuning. Remaining hyper-parameters used their default values as defined by their developers.}
\end{table}

In Table~\ref{table:hyper_params_meta}, we present the optimised hyper-parameters from our meta-learners. For all three instances of modelling (AGN-Galaxy, radio detection, and redshift), hyper-parameters were optimized using the \verb|SkoptSearch| algorithm embedded in the package \verb|tune-sklearn|\footnote{\url{https://github.com/ray-project/tune-sklearn}} \citep[\texttt{v0.4.1};][]{head_tim_2021_5565057}, which implements a bayesian search in the hyper-parameter space.

\section{PR-curve threshold optimisation}\label{sec:app_pr_curve}

\begin{figure}
  \centering
  \begin{minipage}{0.24\textwidth}
    \centering
    \includegraphics[width=0.98\textwidth]{figures/PR_cal_curve_AGN_gal.pdf}\hfill\break
    {(a) AGN-galaxy} 
  \end{minipage}
  \hfill 
  \begin{minipage}{0.24\textwidth}
    \centering
    \includegraphics[width=0.98\textwidth]{figures/PR_cal_curve_radio.pdf}\hfill\break
    {(b) Radio detection}
  \end{minipage}
  \centering
  \caption{Precision-Recall curves for the (a) AGN-Galaxy and (b) radio detection classification models.}
  \label{fig:PR_curves_classification}
\end{figure}

We want to improve the number of recovered elements in each classifier. This improvement implies maximising the recall (Eq.~\ref{eq:recall}), done by decreasing the threshold by which a source is classified as a positive instances. Setting this threshold to its minimum, $0.0$, would help increase the recall. But every source would be predicted to be an AGN or detected on the radio regardless of their properties.% But it will also defeat the purpose of training and applying trained models. Every source would be predicted to be an AGN or to be detected on the radio regardless of their properties.

One statistical tool designed to optimise the classification threshold taking into account the recall but also other properties of the predictions is the Precision-Recall (PR) curve. It can help to understand the behaviour of a classifier as a function of its threshold. Both quantities, precision (Eq.~\ref{eq:precision}) and recall, show an inverse correlation, and both depend on the selected threshold. Thus, they can be used to retrieve the score value for which both quantities are balanced. This optimisation is done by finding the threshold that maximises the $\mathrm{F}_{\beta}$ score (Eq.~\ref{eq:f_beta}), which can also be defined as a slightly modified harmonic mean between precision and recall. This operation will be performed over the union of training and validation sets, which have been used to create and train each model. PR curves for all subsets used in our classification models are shown in Fig.~\ref{fig:PR_curves_classification}.

\section{SHAP values for base models}\label{sec:app_shap_base}

Figures~\ref{fig:SHAP_decision_AGN_base_HETDEX_high_z}, \ref{fig:SHAP_decision_z_base_HETDEX_high_z}, and \ref{fig:SHAP_decision_radio_base_HETDEX_high_z} show the decision plots for each of the base learners used in the prediction models of our pipeline (see Sect.~\ref{sec:shapley_values}). For the classification algorithms, the starting point of their predictions corresponds to the naive threshold ($0.5$) since no threshold optimisation was applied to them and only the scores are included in the stacking step, not the final classification they might provide. In the case of the redshift predictors, the decision plots start at the value $z = 0.0$, as presented for the meta-learner.

\begin{figure}
  \centering
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_AGN_base_xgboost_HETDEX_highz.pdf}
  \end{minipage}%\\%
  \hfill
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_AGN_base_rf_HETDEX_highz.pdf}
  \end{minipage}\break%\%
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_AGN_base_et_HETDEX_highz.pdf}
  \end{minipage}%\\%
  \hfill
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_AGN_base_gbc_HETDEX_highz.pdf}
  \end{minipage}
  \caption{SHAP decision plots for base AGN-Galaxy algorithms. Details as described in Figs.~\ref{fig:SHAP_decision_AGN_meta_HETDEX_high_z}. Starting point of predictions is the naive classification threshold. From left to right and from top to bottom, each panel shows the results from \texttt{XGBoost}, \texttt{RF}, \texttt{ET}, and \texttt{GBC}.}
  \label{fig:SHAP_decision_AGN_base_HETDEX_high_z}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_radio_base_xgboost_HETDEX_highz.pdf}
  \end{minipage}%\\%
  \hfill
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_radio_base_catboost_HETDEX_highz.pdf}
  \end{minipage}\break%\%
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_radio_base_rf_HETDEX_highz.pdf}
  \end{minipage}%\\%
  \hfill
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_radio_base_et_HETDEX_highz.pdf}
  \end{minipage}
  \caption{SHAP decision plots from base radio algorithms. Details as Figs.~\ref{fig:SHAP_decision_AGN_meta_HETDEX_high_z} and \ref{fig:SHAP_decision_AGN_base_HETDEX_high_z}. Each panel with results for \texttt{XGBoost}, \texttt{CatBoost}, \texttt{RF}, and \texttt{ET}.}
  \label{fig:SHAP_decision_radio_base_HETDEX_high_z}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_z_base_rf_HETDEX_highz.pdf}
  \end{minipage}%\\%
  \hfill
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_z_base_catboost_HETDEX_highz.pdf}
  \end{minipage}\break%\%
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_z_base_xgboost_HETDEX_highz.pdf}
  \end{minipage}%\\%
  \hfill
  \begin{minipage}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/SHAP/SHAP_decision_z_base_gbr_HETDEX_highz.pdf}
  \end{minipage}
  \caption{SHAP decision plots from base $z$ algorithms. Details as in Fig~\ref{fig:SHAP_decision_AGN_meta_HETDEX_high_z}. Each panel shows results for \texttt{ET}, \texttt{CatBoost}, \texttt{XGBoost}, and \texttt{GBR}.}
  \label{fig:SHAP_decision_z_base_HETDEX_high_z}
\end{figure}

\section{Prediction results for radio AGN}\label{sec:app_prediction_results}

The columns shown in the prediction results are described.

\begin{itemize}

\item ID: Internal identification number.
\item RA\_ICRS: Right Ascension (in degrees) of source in CW.
\item DE\_ICRS: Declination (in degrees) of source in CW.
%\item Name: Name of the source as it appears in the CW catalogue.
%\item \texttt{TYPE}: Type, or types, of sources according to the MQC (\texttt{Q} = QSO, \texttt{A} = AGN, \texttt{B} = BL Lac, \texttt{L} = lensed quasar, \texttt{K} = NLQSO, \texttt{N} = NLAGN, \texttt{R} = radio association, \texttt{X} = X-ray association, \texttt{2} = double radio lobes).
\item \texttt{band\_num}: Number of non-radio bands with a valid measurement per source (cf. Sect.~\ref{sec:feature_creation}).
\item \texttt{class}: \verb|1| if a source is a confirmed AGN by the MQC. \verb|0| if it has been spectroscopically confirmed as a galaxy in SDSS DR16. Sources with no value do not have a spectroscopic classification in this catalogue.
%\item Pred\_AGN: \verb|1| if a source has been predicted to be an AGN by our model. \verb|0| otherwise.
\item Score\_AGN: Score from the meta AGN-Galaxy classifier for a prediction to be an AGN.
\item Prob\_AGN: Probability from the calibrated meta AGN-Galaxy classifier for a prediction to be an AGN.
\item \texttt{LOFAR\_detect}: \verb|1| if a source has been detected on the LoTSS survey or in their analogue surveys for fields different to HETDEX (see Sect.~\ref{sec:data_collection}). \verb|0| otherwise.
%\item Pred\_radio: \verb|1| if a predicted AGN has been predicted to be detected in the radio by our model. \verb|0| otherwise.
\item Score\_radio: Score from the meta radio detection model for a prediction to be detected in the radio.
\item Prob\_radio: Probability, from the calibrated radio detection model for a prediction to be detected in the radio.
\item Score\_rAGN: Score\_AGN~$\times$~Score\_radio. Score of a source for it to be an AGN detected in the radio.
\item Prob\_rAGN: Prob\_AGN~$\times$~Prob\_radio. Probability of a source for it to be an AGN detected in the radio.
\item \texttt{Z}: Spectroscopic redshift as listed by the MQC (if available).
\item pred\_Z: Redshift value predicted by our model.

\end{itemize}

\end{appendix}

\end{document}

%%%% End of ML_pipeline.tex
